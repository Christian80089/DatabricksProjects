{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56241d3b-1bed-4676-bb21-59bca2e682f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Random CSV and Parquet for Simple ETL Pipeline"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================================\n",
    "# DATA GENERATION SCRIPT FOR ETL PIPELINE TESTING\n",
    "# ============================================================================\n",
    "# This script generates synthetic sales data in CSV and Parquet formats\n",
    "# Run this notebook before executing the main ETL pipeline\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SYNTHETIC DATA GENERATOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Number of records to generate\n",
    "NUM_RECORDS = 280000\n",
    "\n",
    "# Output paths (matching the ETL notebook)\n",
    "OUTPUT_CSV_PATH = f\"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/raw_sales_data_{datetime.now()}.csv\"\n",
    "OUTPUT_PARQUET_PATH = f\"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/raw_sales_data_{datetime.now()}.parquet\"\n",
    "\n",
    "# Seed for reproducibility (optional)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Records to generate: {NUM_RECORDS:,}\")\n",
    "print(f\"  - CSV output: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  - Parquet output: {OUTPUT_PARQUET_PATH}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# DATA GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Define possible values for categorical fields\n",
    "CATEGORIES = [\n",
    "    \"Electronics\", \"Clothing\", \"Home & Garden\", \"Sports\", \n",
    "    \"Books\", \"Toys\", \"Automotive\", \"Health & Beauty\"\n",
    "]\n",
    "\n",
    "REGIONS = [\n",
    "    \"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\", \n",
    "    \"Middle East\", \"Africa\"\n",
    "]\n",
    "\n",
    "FIRST_NAMES = [\n",
    "    \"John\", \"Mary\", \"David\", \"Sarah\", \"Michael\", \"Emma\", \"James\", \n",
    "    \"Linda\", \"Robert\", \"Patricia\", \"William\", \"Jennifer\", \"Richard\",\n",
    "    \"Elizabeth\", \"Thomas\", \"Maria\", \"Charles\", \"Susan\", \"Daniel\", \"Jessica\"\n",
    "]\n",
    "\n",
    "LAST_NAMES = [\n",
    "    \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \n",
    "    \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\", \"Hernandez\", \n",
    "    \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \"Thomas\", \"Taylor\"\n",
    "]\n",
    "\n",
    "SALES_PERSONS = [\n",
    "    \"Alice Johnson\", \"Bob Smith\", \"Carol Davis\", \"Dan Brown\",\n",
    "    \"Eve Wilson\", \"Frank Miller\", \"Grace Lee\", \"Henry Taylor\"\n",
    "]\n",
    "\n",
    "PRODUCT_PREFIXES = {\n",
    "    \"Electronics\": [\"Laptop\", \"Smartphone\", \"Tablet\", \"Headphones\", \"Camera\"],\n",
    "    \"Clothing\": [\"T-Shirt\", \"Jeans\", \"Jacket\", \"Shoes\", \"Dress\"],\n",
    "    \"Home & Garden\": [\"Lamp\", \"Chair\", \"Table\", \"Curtains\", \"Plant\"],\n",
    "    \"Sports\": [\"Basketball\", \"Soccer Ball\", \"Tennis Racket\", \"Yoga Mat\", \"Dumbbell\"],\n",
    "    \"Books\": [\"Novel\", \"Textbook\", \"Cookbook\", \"Biography\", \"Guide\"],\n",
    "    \"Toys\": [\"Action Figure\", \"Board Game\", \"Puzzle\", \"Doll\", \"Building Set\"],\n",
    "    \"Automotive\": [\"Car Part\", \"Oil Filter\", \"Tire\", \"Battery\", \"Wiper\"],\n",
    "    \"Health & Beauty\": [\"Shampoo\", \"Lotion\", \"Vitamin\", \"Perfume\", \"Soap\"]\n",
    "}\n",
    "\n",
    "# Date range for transactions (last 2 years)\n",
    "END_DATE = datetime(2026, 1, 31)\n",
    "START_DATE = END_DATE - timedelta(days=730)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_transaction_id(index):\n",
    "    \"\"\"Generate unique transaction ID\"\"\"\n",
    "    return f\"TXN{str(index).zfill(8)}\"\n",
    "\n",
    "def generate_random_date(start_date, end_date):\n",
    "    \"\"\"Generate random date between start and end\"\"\"\n",
    "    time_delta = end_date - start_date\n",
    "    random_days = random.randint(0, time_delta.days)\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def generate_customer_name():\n",
    "    \"\"\"Generate random customer name\"\"\"\n",
    "    first = random.choice(FIRST_NAMES)\n",
    "    last = random.choice(LAST_NAMES)\n",
    "    return f\"{first} {last}\"\n",
    "\n",
    "def generate_product_name(category):\n",
    "    \"\"\"Generate product name based on category\"\"\"\n",
    "    prefix = random.choice(PRODUCT_PREFIXES[category])\n",
    "    model = random.choice([\"Pro\", \"Plus\", \"Standard\", \"Elite\", \"Basic\"])\n",
    "    number = random.randint(100, 999)\n",
    "    return f\"{prefix} {model} {number}\"\n",
    "\n",
    "def generate_unit_price(category):\n",
    "    \"\"\"Generate realistic price based on category\"\"\"\n",
    "    price_ranges = {\n",
    "        \"Electronics\": (50, 2000),\n",
    "        \"Clothing\": (15, 200),\n",
    "        \"Home & Garden\": (20, 500),\n",
    "        \"Sports\": (10, 300),\n",
    "        \"Books\": (5, 50),\n",
    "        \"Toys\": (10, 100),\n",
    "        \"Automotive\": (25, 500),\n",
    "        \"Health & Beauty\": (5, 150)\n",
    "    }\n",
    "    min_price, max_price = price_ranges[category]\n",
    "    return round(random.uniform(min_price, max_price), 2)\n",
    "\n",
    "def introduce_data_quality_issues(data, issue_rate=0.05):\n",
    "    \"\"\"\n",
    "    Introduce realistic data quality issues for testing\n",
    "    - Missing values\n",
    "    - Duplicates\n",
    "    - Inconsistent formatting\n",
    "    - Invalid values\n",
    "    \"\"\"\n",
    "    # Introduce some null values (5% chance)\n",
    "    if random.random() < issue_rate:\n",
    "        field = random.choice(['customer_name', 'category', 'region', 'sales_person'])\n",
    "        data[field] = None\n",
    "    \n",
    "    # Introduce inconsistent spacing/casing (3% chance)\n",
    "    if random.random() < (issue_rate * 0.6):\n",
    "        if data['customer_name']:\n",
    "            data['customer_name'] = \"  \" + data['customer_name'].lower() + \"  \"\n",
    "    \n",
    "    # Introduce negative quantity (1% chance)\n",
    "    if random.random() < (issue_rate * 0.2):\n",
    "        data['quantity'] = -1 * random.randint(1, 5)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE SYNTHETIC DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating synthetic sales data...\")\n",
    "\n",
    "# List to store all records\n",
    "sales_data = []\n",
    "\n",
    "# Generate records\n",
    "for i in range(1, NUM_RECORDS + 1):\n",
    "    # Select random category\n",
    "    category = random.choice(CATEGORIES)\n",
    "    \n",
    "    # Generate base record\n",
    "    record = {\n",
    "        \"transaction_id\": generate_transaction_id(i),\n",
    "        \"transaction_date\": generate_random_date(START_DATE, END_DATE),\n",
    "        \"customer_id\": f\"CUST{str(random.randint(1, 5000)).zfill(6)}\",\n",
    "        \"customer_name\": generate_customer_name(),\n",
    "        \"product_id\": f\"PROD{str(random.randint(1, 1000)).zfill(5)}\",\n",
    "        \"product_name\": generate_product_name(category),\n",
    "        \"category\": category,\n",
    "        \"quantity\": random.randint(1, 10),\n",
    "        \"unit_price\": generate_unit_price(category),\n",
    "        \"discount\": round(random.choice([0, 0, 0, 0.05, 0.1, 0.15, 0.2, 0.25]), 2),\n",
    "        \"region\": random.choice(REGIONS),\n",
    "        \"sales_person\": random.choice(SALES_PERSONS)\n",
    "    }\n",
    "    \n",
    "    # Introduce data quality issues in some records\n",
    "    record = introduce_data_quality_issues(record, issue_rate=0.05)\n",
    "    \n",
    "    sales_data.append(record)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Generated {i:,} records...\")\n",
    "\n",
    "print(f\"âœ“ Generated {NUM_RECORDS:,} records\")\n",
    "\n",
    "# Add some duplicate records (2% of total)\n",
    "num_duplicates = int(NUM_RECORDS * 0.02)\n",
    "duplicates = random.sample(sales_data, num_duplicates)\n",
    "sales_data.extend(duplicates)\n",
    "print(f\"âœ“ Added {num_duplicates} duplicate records for quality testing\")\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(sales_data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS CSV\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nSaving data as CSV...\")\n",
    "\n",
    "# Convert to pandas DataFrame for easy CSV export\n",
    "df_pandas = pd.DataFrame(sales_data)\n",
    "\n",
    "# Reorder columns to match schema\n",
    "column_order = [\n",
    "    \"transaction_id\", \"transaction_date\", \"customer_id\", \"customer_name\",\n",
    "    \"product_id\", \"product_name\", \"category\", \"quantity\", \n",
    "    \"unit_price\", \"discount\", \"region\", \"sales_person\"\n",
    "]\n",
    "df_pandas = df_pandas[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "df_pandas.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"âœ“ CSV file saved successfully\")\n",
    "print(f\"  - Path: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  - Size: {len(sales_data):,} rows x {len(column_order)} columns\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of generated CSV data:\")\n",
    "print(df_pandas.head(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS PARQUET\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nSaving data as Parquet...\")\n",
    "\n",
    "# Define schema for Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"sales_person\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_spark = spark.createDataFrame(sales_data, schema=schema)\n",
    "\n",
    "# Save as Parquet\n",
    "df_spark.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(OUTPUT_PARQUET_PATH)\n",
    "\n",
    "print(f\"âœ“ Parquet file saved successfully\")\n",
    "print(f\"  - Path: {OUTPUT_PARQUET_PATH}\")\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nParquet schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# DATA VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA GENERATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read back and validate CSV\n",
    "df_csv_validate = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(OUTPUT_CSV_PATH)\n",
    "\n",
    "# Read back and validate Parquet\n",
    "df_parquet_validate = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(OUTPUT_PARQUET_PATH)\n",
    "\n",
    "print(f\"\\nâœ“ CSV Validation:\")\n",
    "print(f\"  - Rows: {df_csv_validate.count():,}\")\n",
    "print(f\"  - Columns: {len(df_csv_validate.columns)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Parquet Validation:\")\n",
    "print(f\"  - Rows: {df_parquet_validate.count():,}\")\n",
    "print(f\"  - Columns: {len(df_parquet_validate.columns)}\")\n",
    "\n",
    "# Data quality summary\n",
    "print(f\"\\nâœ“ Data Quality Characteristics:\")\n",
    "print(f\"  - Clean records: ~{int(NUM_RECORDS * 0.95):,}\")\n",
    "print(f\"  - Records with issues: ~{int(NUM_RECORDS * 0.05):,}\")\n",
    "print(f\"  - Duplicate records: ~{num_duplicates:,}\")\n",
    "print(f\"  - Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"  - Categories: {len(CATEGORIES)}\")\n",
    "print(f\"  - Regions: {len(REGIONS)}\")\n",
    "\n",
    "# Category distribution\n",
    "print(f\"\\nâœ“ Category Distribution:\")\n",
    "category_counts = df_parquet_validate.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
    "display(category_counts)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MESSAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… DATA GENERATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerated files are ready for ETL pipeline:\")\n",
    "print(f\"  ðŸ“„ CSV: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  ðŸ“¦ Parquet: {OUTPUT_PARQUET_PATH}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Run the main ETL notebook\")\n",
    "print(f\"  2. The pipeline will read these files as source data\")\n",
    "print(f\"  3. Data quality issues will be cleaned during transformation\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
