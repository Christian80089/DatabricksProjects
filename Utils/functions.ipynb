{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "56241d3b-1bed-4676-bb21-59bca2e682f1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Generate Random CSV and Parquet for Simple ETL Pipeline"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\nSYNTHETIC DATA GENERATOR\n================================================================================\nConfiguration:\n  - Records to generate: 280,000\n  - CSV output: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/raw_sales_data_2026-02-02 18:42:54.147842.csv\n  - Parquet output: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/raw_sales_data_2026-02-02 18:42:54.147881.parquet\n\nGenerating synthetic sales data...\n  Generated 1,000 records...\n  Generated 2,000 records...\n  Generated 3,000 records...\n  Generated 4,000 records...\n  Generated 5,000 records...\n  Generated 6,000 records...\n  Generated 7,000 records...\n  Generated 8,000 records...\n  Generated 9,000 records...\n  Generated 10,000 records...\n  Generated 11,000 records...\n  Generated 12,000 records...\n  Generated 13,000 records...\n  Generated 14,000 records...\n  Generated 15,000 records...\n  Generated 16,000 records...\n  Generated 17,000 records...\n  Generated 18,000 records...\n  Generated 19,000 records...\n  Generated 20,000 records...\n  Generated 21,000 records...\n  Generated 22,000 records...\n  Generated 23,000 records...\n  Generated 24,000 records...\n  Generated 25,000 records...\n  Generated 26,000 records...\n  Generated 27,000 records...\n  Generated 28,000 records...\n  Generated 29,000 records...\n  Generated 30,000 records...\n  Generated 31,000 records...\n  Generated 32,000 records...\n  Generated 33,000 records...\n  Generated 34,000 records...\n  Generated 35,000 records...\n  Generated 36,000 records...\n  Generated 37,000 records...\n  Generated 38,000 records...\n  Generated 39,000 records...\n  Generated 40,000 records...\n  Generated 41,000 records...\n  Generated 42,000 records...\n  Generated 43,000 records...\n  Generated 44,000 records...\n  Generated 45,000 records...\n  Generated 46,000 records...\n  Generated 47,000 records...\n  Generated 48,000 records...\n  Generated 49,000 records...\n  Generated 50,000 records...\n  Generated 51,000 records...\n  Generated 52,000 records...\n  Generated 53,000 records...\n  Generated 54,000 records...\n  Generated 55,000 records...\n  Generated 56,000 records...\n  Generated 57,000 records...\n  Generated 58,000 records...\n  Generated 59,000 records...\n  Generated 60,000 records...\n  Generated 61,000 records...\n  Generated 62,000 records...\n  Generated 63,000 records...\n  Generated 64,000 records...\n  Generated 65,000 records...\n  Generated 66,000 records...\n  Generated 67,000 records...\n  Generated 68,000 records...\n  Generated 69,000 records...\n  Generated 70,000 records...\n  Generated 71,000 records...\n  Generated 72,000 records...\n  Generated 73,000 records...\n  Generated 74,000 records...\n  Generated 75,000 records...\n  Generated 76,000 records...\n  Generated 77,000 records...\n  Generated 78,000 records...\n  Generated 79,000 records...\n  Generated 80,000 records...\n  Generated 81,000 records...\n  Generated 82,000 records...\n  Generated 83,000 records...\n  Generated 84,000 records...\n  Generated 85,000 records...\n  Generated 86,000 records...\n  Generated 87,000 records...\n  Generated 88,000 records...\n  Generated 89,000 records...\n  Generated 90,000 records...\n  Generated 91,000 records...\n  Generated 92,000 records...\n  Generated 93,000 records...\n  Generated 94,000 records...\n  Generated 95,000 records...\n  Generated 96,000 records...\n  Generated 97,000 records...\n  Generated 98,000 records...\n  Generated 99,000 records...\n  Generated 100,000 records...\n  Generated 101,000 records...\n  Generated 102,000 records...\n  Generated 103,000 records...\n  Generated 104,000 records...\n  Generated 105,000 records...\n  Generated 106,000 records...\n  Generated 107,000 records...\n  Generated 108,000 records...\n  Generated 109,000 records...\n  Generated 110,000 records...\n  Generated 111,000 records...\n  Generated 112,000 records...\n  Generated 113,000 records...\n  Generated 114,000 records...\n  Generated 115,000 records...\n  Generated 116,000 records...\n  Generated 117,000 records...\n  Generated 118,000 records...\n  Generated 119,000 records...\n  Generated 120,000 records...\n  Generated 121,000 records...\n  Generated 122,000 records...\n  Generated 123,000 records...\n  Generated 124,000 records...\n  Generated 125,000 records...\n  Generated 126,000 records...\n  Generated 127,000 records...\n  Generated 128,000 records...\n  Generated 129,000 records...\n  Generated 130,000 records...\n  Generated 131,000 records...\n  Generated 132,000 records...\n  Generated 133,000 records...\n  Generated 134,000 records...\n  Generated 135,000 records...\n  Generated 136,000 records...\n  Generated 137,000 records...\n  Generated 138,000 records...\n  Generated 139,000 records...\n  Generated 140,000 records...\n  Generated 141,000 records...\n  Generated 142,000 records...\n  Generated 143,000 records...\n  Generated 144,000 records...\n  Generated 145,000 records...\n  Generated 146,000 records...\n  Generated 147,000 records...\n  Generated 148,000 records...\n  Generated 149,000 records...\n  Generated 150,000 records...\n  Generated 151,000 records...\n  Generated 152,000 records...\n  Generated 153,000 records...\n  Generated 154,000 records...\n  Generated 155,000 records...\n  Generated 156,000 records...\n  Generated 157,000 records...\n  Generated 158,000 records...\n  Generated 159,000 records...\n  Generated 160,000 records...\n  Generated 161,000 records...\n  Generated 162,000 records...\n  Generated 163,000 records...\n  Generated 164,000 records...\n  Generated 165,000 records...\n  Generated 166,000 records...\n  Generated 167,000 records...\n  Generated 168,000 records...\n  Generated 169,000 records...\n  Generated 170,000 records...\n  Generated 171,000 records...\n  Generated 172,000 records...\n  Generated 173,000 records...\n  Generated 174,000 records...\n  Generated 175,000 records...\n  Generated 176,000 records...\n  Generated 177,000 records...\n  Generated 178,000 records...\n  Generated 179,000 records...\n  Generated 180,000 records...\n  Generated 181,000 records...\n  Generated 182,000 records...\n  Generated 183,000 records...\n  Generated 184,000 records...\n  Generated 185,000 records...\n  Generated 186,000 records...\n  Generated 187,000 records...\n  Generated 188,000 records...\n  Generated 189,000 records...\n  Generated 190,000 records...\n  Generated 191,000 records...\n  Generated 192,000 records...\n  Generated 193,000 records...\n  Generated 194,000 records...\n  Generated 195,000 records...\n  Generated 196,000 records...\n  Generated 197,000 records...\n  Generated 198,000 records...\n  Generated 199,000 records...\n  Generated 200,000 records...\n  Generated 201,000 records...\n  Generated 202,000 records...\n  Generated 203,000 records...\n  Generated 204,000 records...\n  Generated 205,000 records...\n  Generated 206,000 records...\n  Generated 207,000 records...\n  Generated 208,000 records...\n  Generated 209,000 records...\n  Generated 210,000 records...\n  Generated 211,000 records...\n  Generated 212,000 records...\n  Generated 213,000 records...\n  Generated 214,000 records...\n  Generated 215,000 records...\n  Generated 216,000 records...\n  Generated 217,000 records...\n  Generated 218,000 records...\n  Generated 219,000 records...\n  Generated 220,000 records...\n  Generated 221,000 records...\n  Generated 222,000 records...\n  Generated 223,000 records...\n  Generated 224,000 records...\n  Generated 225,000 records...\n  Generated 226,000 records...\n  Generated 227,000 records...\n  Generated 228,000 records...\n  Generated 229,000 records...\n  Generated 230,000 records...\n  Generated 231,000 records...\n  Generated 232,000 records...\n  Generated 233,000 records...\n  Generated 234,000 records...\n  Generated 235,000 records...\n  Generated 236,000 records...\n  Generated 237,000 records...\n  Generated 238,000 records...\n  Generated 239,000 records...\n  Generated 240,000 records...\n  Generated 241,000 records...\n  Generated 242,000 records...\n  Generated 243,000 records...\n  Generated 244,000 records...\n  Generated 245,000 records...\n  Generated 246,000 records...\n  Generated 247,000 records...\n  Generated 248,000 records...\n  Generated 249,000 records...\n  Generated 250,000 records...\n  Generated 251,000 records...\n  Generated 252,000 records...\n  Generated 253,000 records...\n  Generated 254,000 records...\n  Generated 255,000 records...\n  Generated 256,000 records...\n  Generated 257,000 records...\n  Generated 258,000 records...\n  Generated 259,000 records...\n  Generated 260,000 records...\n  Generated 261,000 records...\n  Generated 262,000 records...\n  Generated 263,000 records...\n  Generated 264,000 records...\n  Generated 265,000 records...\n  Generated 266,000 records...\n  Generated 267,000 records...\n  Generated 268,000 records...\n  Generated 269,000 records...\n  Generated 270,000 records...\n  Generated 271,000 records...\n  Generated 272,000 records...\n  Generated 273,000 records...\n  Generated 274,000 records...\n  Generated 275,000 records...\n  Generated 276,000 records...\n  Generated 277,000 records...\n  Generated 278,000 records...\n  Generated 279,000 records...\n  Generated 280,000 records...\nâœ“ Generated 280,000 records\nâœ“ Added 5600 duplicate records for quality testing\n\nSaving data as CSV...\nâœ“ CSV file saved successfully\n  - Path: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/raw_sales_data_2026-02-02 18:42:54.147842.csv\n  - Size: 285,600 rows x 12 columns\n\nSample of generated CSV data:\n  transaction_id transaction_date  ...         region   sales_person\n0    TXN00108733       2025-02-04  ...  North America      Dan Brown\n1    TXN00073781       2024-09-01  ...         Africa    Carol Davis\n2    TXN00175181       2025-04-25  ...   Asia Pacific  Alice Johnson\n3    TXN00113769       2025-03-12  ...   Asia Pacific   Frank Miller\n4    TXN00249584       2025-10-29  ...  Latin America    Carol Davis\n5    TXN00168394       2025-02-21  ...         Europe      Bob Smith\n6    TXN00241193       2024-04-25  ...  Latin America   Henry Taylor\n7    TXN00274108       2024-06-09  ...  North America   Frank Miller\n8    TXN00077601       2025-03-27  ...  North America      Bob Smith\n9    TXN00063311       2026-01-15  ...         Africa   Frank Miller\n\n[10 rows x 12 columns]\n\nSaving data as Parquet...\nâœ“ Parquet file saved successfully\n  - Path: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/raw_sales_data_2026-02-02 18:42:54.147881.parquet\n\nParquet schema:\nroot\n |-- transaction_id: string (nullable = true)\n |-- transaction_date: string (nullable = true)\n |-- customer_id: string (nullable = true)\n |-- customer_name: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- product_name: string (nullable = true)\n |-- category: string (nullable = true)\n |-- quantity: integer (nullable = true)\n |-- unit_price: double (nullable = true)\n |-- discount: double (nullable = true)\n |-- region: string (nullable = true)\n |-- sales_person: string (nullable = true)\n\n\n================================================================================\nDATA GENERATION SUMMARY\n================================================================================\n\nâœ“ CSV Validation:\n  - Rows: 285,600\n  - Columns: 12\n\nâœ“ Parquet Validation:\n  - Rows: 285,600\n  - Columns: 12\n\nâœ“ Data Quality Characteristics:\n  - Clean records: ~266,000\n  - Records with issues: ~14,000\n  - Duplicate records: ~5,600\n  - Date range: 2024-02-01 to 2026-01-31\n  - Categories: 8\n  - Regions: 6\n\nâœ“ Category Distribution:\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>count</th></tr></thead><tbody><tr><td>Automotive</td><td>35481</td></tr><tr><td>Health & Beauty</td><td>35472</td></tr><tr><td>Clothing</td><td>35357</td></tr><tr><td>Home & Garden</td><td>35279</td></tr><tr><td>Electronics</td><td>35210</td></tr><tr><td>Toys</td><td>35151</td></tr><tr><td>Sports</td><td>35115</td></tr><tr><td>Books</td><td>34991</td></tr><tr><td>null</td><td>3544</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Automotive",
         35481
        ],
        [
         "Health & Beauty",
         35472
        ],
        [
         "Clothing",
         35357
        ],
        [
         "Home & Garden",
         35279
        ],
        [
         "Electronics",
         35210
        ],
        [
         "Toys",
         35151
        ],
        [
         "Sports",
         35115
        ],
        [
         "Books",
         34991
        ],
        [
         null,
         3544
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n================================================================================\nâœ… DATA GENERATION COMPLETED SUCCESSFULLY\n================================================================================\n\nGenerated files are ready for ETL pipeline:\n  ðŸ“„ CSV: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/raw_sales_data_2026-02-02 18:42:54.147842.csv\n  ðŸ“¦ Parquet: /Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/raw_sales_data_2026-02-02 18:42:54.147881.parquet\n\nNext steps:\n  1. Run the main ETL notebook\n  2. The pipeline will read these files as source data\n  3. Data quality issues will be cleaned during transformation\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================================\n",
    "# DATA GENERATION SCRIPT FOR ETL PIPELINE TESTING\n",
    "# ============================================================================\n",
    "# This script generates synthetic sales data in CSV and Parquet formats\n",
    "# Run this notebook before executing the main ETL pipeline\n",
    "\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"SYNTHETIC DATA GENERATOR\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# CONFIGURATION\n",
    "# ============================================================================\n",
    "\n",
    "# Number of records to generate\n",
    "NUM_RECORDS = 280000\n",
    "\n",
    "# Output paths (matching the ETL notebook)\n",
    "OUTPUT_CSV_PATH = f\"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/raw_sales_data_{datetime.now()}.csv\"\n",
    "OUTPUT_PARQUET_PATH = f\"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/raw_sales_data_{datetime.now()}.parquet\"\n",
    "\n",
    "# Seed for reproducibility (optional)\n",
    "random.seed(42)\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  - Records to generate: {NUM_RECORDS:,}\")\n",
    "print(f\"  - CSV output: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  - Parquet output: {OUTPUT_PARQUET_PATH}\")\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# DATA GENERATION PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Define possible values for categorical fields\n",
    "CATEGORIES = [\n",
    "    \"Electronics\", \"Clothing\", \"Home & Garden\", \"Sports\", \n",
    "    \"Books\", \"Toys\", \"Automotive\", \"Health & Beauty\"\n",
    "]\n",
    "\n",
    "REGIONS = [\n",
    "    \"North America\", \"Europe\", \"Asia Pacific\", \"Latin America\", \n",
    "    \"Middle East\", \"Africa\"\n",
    "]\n",
    "\n",
    "FIRST_NAMES = [\n",
    "    \"John\", \"Mary\", \"David\", \"Sarah\", \"Michael\", \"Emma\", \"James\", \n",
    "    \"Linda\", \"Robert\", \"Patricia\", \"William\", \"Jennifer\", \"Richard\",\n",
    "    \"Elizabeth\", \"Thomas\", \"Maria\", \"Charles\", \"Susan\", \"Daniel\", \"Jessica\"\n",
    "]\n",
    "\n",
    "LAST_NAMES = [\n",
    "    \"Smith\", \"Johnson\", \"Williams\", \"Brown\", \"Jones\", \"Garcia\", \n",
    "    \"Miller\", \"Davis\", \"Rodriguez\", \"Martinez\", \"Hernandez\", \n",
    "    \"Lopez\", \"Gonzalez\", \"Wilson\", \"Anderson\", \"Thomas\", \"Taylor\"\n",
    "]\n",
    "\n",
    "SALES_PERSONS = [\n",
    "    \"Alice Johnson\", \"Bob Smith\", \"Carol Davis\", \"Dan Brown\",\n",
    "    \"Eve Wilson\", \"Frank Miller\", \"Grace Lee\", \"Henry Taylor\"\n",
    "]\n",
    "\n",
    "PRODUCT_PREFIXES = {\n",
    "    \"Electronics\": [\"Laptop\", \"Smartphone\", \"Tablet\", \"Headphones\", \"Camera\"],\n",
    "    \"Clothing\": [\"T-Shirt\", \"Jeans\", \"Jacket\", \"Shoes\", \"Dress\"],\n",
    "    \"Home & Garden\": [\"Lamp\", \"Chair\", \"Table\", \"Curtains\", \"Plant\"],\n",
    "    \"Sports\": [\"Basketball\", \"Soccer Ball\", \"Tennis Racket\", \"Yoga Mat\", \"Dumbbell\"],\n",
    "    \"Books\": [\"Novel\", \"Textbook\", \"Cookbook\", \"Biography\", \"Guide\"],\n",
    "    \"Toys\": [\"Action Figure\", \"Board Game\", \"Puzzle\", \"Doll\", \"Building Set\"],\n",
    "    \"Automotive\": [\"Car Part\", \"Oil Filter\", \"Tire\", \"Battery\", \"Wiper\"],\n",
    "    \"Health & Beauty\": [\"Shampoo\", \"Lotion\", \"Vitamin\", \"Perfume\", \"Soap\"]\n",
    "}\n",
    "\n",
    "# Date range for transactions (last 2 years)\n",
    "END_DATE = datetime(2026, 1, 31)\n",
    "START_DATE = END_DATE - timedelta(days=730)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# HELPER FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def generate_transaction_id(index):\n",
    "    \"\"\"Generate unique transaction ID\"\"\"\n",
    "    return f\"TXN{str(index).zfill(8)}\"\n",
    "\n",
    "def generate_random_date(start_date, end_date):\n",
    "    \"\"\"Generate random date between start and end\"\"\"\n",
    "    time_delta = end_date - start_date\n",
    "    random_days = random.randint(0, time_delta.days)\n",
    "    return (start_date + timedelta(days=random_days)).strftime(\"%Y-%m-%d\")\n",
    "\n",
    "def generate_customer_name():\n",
    "    \"\"\"Generate random customer name\"\"\"\n",
    "    first = random.choice(FIRST_NAMES)\n",
    "    last = random.choice(LAST_NAMES)\n",
    "    return f\"{first} {last}\"\n",
    "\n",
    "def generate_product_name(category):\n",
    "    \"\"\"Generate product name based on category\"\"\"\n",
    "    prefix = random.choice(PRODUCT_PREFIXES[category])\n",
    "    model = random.choice([\"Pro\", \"Plus\", \"Standard\", \"Elite\", \"Basic\"])\n",
    "    number = random.randint(100, 999)\n",
    "    return f\"{prefix} {model} {number}\"\n",
    "\n",
    "def generate_unit_price(category):\n",
    "    \"\"\"Generate realistic price based on category\"\"\"\n",
    "    price_ranges = {\n",
    "        \"Electronics\": (50, 2000),\n",
    "        \"Clothing\": (15, 200),\n",
    "        \"Home & Garden\": (20, 500),\n",
    "        \"Sports\": (10, 300),\n",
    "        \"Books\": (5, 50),\n",
    "        \"Toys\": (10, 100),\n",
    "        \"Automotive\": (25, 500),\n",
    "        \"Health & Beauty\": (5, 150)\n",
    "    }\n",
    "    min_price, max_price = price_ranges[category]\n",
    "    return round(random.uniform(min_price, max_price), 2)\n",
    "\n",
    "def introduce_data_quality_issues(data, issue_rate=0.05):\n",
    "    \"\"\"\n",
    "    Introduce realistic data quality issues for testing\n",
    "    - Missing values\n",
    "    - Duplicates\n",
    "    - Inconsistent formatting\n",
    "    - Invalid values\n",
    "    \"\"\"\n",
    "    # Introduce some null values (5% chance)\n",
    "    if random.random() < issue_rate:\n",
    "        field = random.choice(['customer_name', 'category', 'region', 'sales_person'])\n",
    "        data[field] = None\n",
    "    \n",
    "    # Introduce inconsistent spacing/casing (3% chance)\n",
    "    if random.random() < (issue_rate * 0.6):\n",
    "        if data['customer_name']:\n",
    "            data['customer_name'] = \"  \" + data['customer_name'].lower() + \"  \"\n",
    "    \n",
    "    # Introduce negative quantity (1% chance)\n",
    "    if random.random() < (issue_rate * 0.2):\n",
    "        data['quantity'] = -1 * random.randint(1, 5)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# GENERATE SYNTHETIC DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nGenerating synthetic sales data...\")\n",
    "\n",
    "# List to store all records\n",
    "sales_data = []\n",
    "\n",
    "# Generate records\n",
    "for i in range(1, NUM_RECORDS + 1):\n",
    "    # Select random category\n",
    "    category = random.choice(CATEGORIES)\n",
    "    \n",
    "    # Generate base record\n",
    "    record = {\n",
    "        \"transaction_id\": generate_transaction_id(i),\n",
    "        \"transaction_date\": generate_random_date(START_DATE, END_DATE),\n",
    "        \"customer_id\": f\"CUST{str(random.randint(1, 5000)).zfill(6)}\",\n",
    "        \"customer_name\": generate_customer_name(),\n",
    "        \"product_id\": f\"PROD{str(random.randint(1, 1000)).zfill(5)}\",\n",
    "        \"product_name\": generate_product_name(category),\n",
    "        \"category\": category,\n",
    "        \"quantity\": random.randint(1, 10),\n",
    "        \"unit_price\": generate_unit_price(category),\n",
    "        \"discount\": round(random.choice([0, 0, 0, 0.05, 0.1, 0.15, 0.2, 0.25]), 2),\n",
    "        \"region\": random.choice(REGIONS),\n",
    "        \"sales_person\": random.choice(SALES_PERSONS)\n",
    "    }\n",
    "    \n",
    "    # Introduce data quality issues in some records\n",
    "    record = introduce_data_quality_issues(record, issue_rate=0.05)\n",
    "    \n",
    "    sales_data.append(record)\n",
    "    \n",
    "    # Progress indicator\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"  Generated {i:,} records...\")\n",
    "\n",
    "print(f\"âœ“ Generated {NUM_RECORDS:,} records\")\n",
    "\n",
    "# Add some duplicate records (2% of total)\n",
    "num_duplicates = int(NUM_RECORDS * 0.02)\n",
    "duplicates = random.sample(sales_data, num_duplicates)\n",
    "sales_data.extend(duplicates)\n",
    "print(f\"âœ“ Added {num_duplicates} duplicate records for quality testing\")\n",
    "\n",
    "# Shuffle the data\n",
    "random.shuffle(sales_data)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS CSV\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nSaving data as CSV...\")\n",
    "\n",
    "# Convert to pandas DataFrame for easy CSV export\n",
    "df_pandas = pd.DataFrame(sales_data)\n",
    "\n",
    "# Reorder columns to match schema\n",
    "column_order = [\n",
    "    \"transaction_id\", \"transaction_date\", \"customer_id\", \"customer_name\",\n",
    "    \"product_id\", \"product_name\", \"category\", \"quantity\", \n",
    "    \"unit_price\", \"discount\", \"region\", \"sales_person\"\n",
    "]\n",
    "df_pandas = df_pandas[column_order]\n",
    "\n",
    "# Save to CSV\n",
    "df_pandas.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "\n",
    "print(f\"âœ“ CSV file saved successfully\")\n",
    "print(f\"  - Path: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  - Size: {len(sales_data):,} rows x {len(column_order)} columns\")\n",
    "\n",
    "# Display sample\n",
    "print(\"\\nSample of generated CSV data:\")\n",
    "print(df_pandas.head(10))\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# SAVE AS PARQUET\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nSaving data as Parquet...\")\n",
    "\n",
    "# Define schema for Spark DataFrame\n",
    "schema = StructType([\n",
    "    StructField(\"transaction_id\", StringType(), True),\n",
    "    StructField(\"transaction_date\", StringType(), True),\n",
    "    StructField(\"customer_id\", StringType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"product_id\", StringType(), True),\n",
    "    StructField(\"product_name\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True),\n",
    "    StructField(\"unit_price\", DoubleType(), True),\n",
    "    StructField(\"discount\", DoubleType(), True),\n",
    "    StructField(\"region\", StringType(), True),\n",
    "    StructField(\"sales_person\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create Spark DataFrame\n",
    "df_spark = spark.createDataFrame(sales_data, schema=schema)\n",
    "\n",
    "# Save as Parquet\n",
    "df_spark.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(OUTPUT_PARQUET_PATH)\n",
    "\n",
    "print(f\"âœ“ Parquet file saved successfully\")\n",
    "print(f\"  - Path: {OUTPUT_PARQUET_PATH}\")\n",
    "\n",
    "# Display schema\n",
    "print(\"\\nParquet schema:\")\n",
    "df_spark.printSchema()\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# DATA VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"DATA GENERATION SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Read back and validate CSV\n",
    "df_csv_validate = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(OUTPUT_CSV_PATH)\n",
    "\n",
    "# Read back and validate Parquet\n",
    "df_parquet_validate = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .load(OUTPUT_PARQUET_PATH)\n",
    "\n",
    "print(f\"\\nâœ“ CSV Validation:\")\n",
    "print(f\"  - Rows: {df_csv_validate.count():,}\")\n",
    "print(f\"  - Columns: {len(df_csv_validate.columns)}\")\n",
    "\n",
    "print(f\"\\nâœ“ Parquet Validation:\")\n",
    "print(f\"  - Rows: {df_parquet_validate.count():,}\")\n",
    "print(f\"  - Columns: {len(df_parquet_validate.columns)}\")\n",
    "\n",
    "# Data quality summary\n",
    "print(f\"\\nâœ“ Data Quality Characteristics:\")\n",
    "print(f\"  - Clean records: ~{int(NUM_RECORDS * 0.95):,}\")\n",
    "print(f\"  - Records with issues: ~{int(NUM_RECORDS * 0.05):,}\")\n",
    "print(f\"  - Duplicate records: ~{num_duplicates:,}\")\n",
    "print(f\"  - Date range: {START_DATE.date()} to {END_DATE.date()}\")\n",
    "print(f\"  - Categories: {len(CATEGORIES)}\")\n",
    "print(f\"  - Regions: {len(REGIONS)}\")\n",
    "\n",
    "# Category distribution\n",
    "print(f\"\\nâœ“ Category Distribution:\")\n",
    "category_counts = df_parquet_validate.groupBy(\"category\").count().orderBy(\"count\", ascending=False)\n",
    "display(category_counts)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL MESSAGE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… DATA GENERATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nGenerated files are ready for ETL pipeline:\")\n",
    "print(f\"  ðŸ“„ CSV: {OUTPUT_CSV_PATH}\")\n",
    "print(f\"  ðŸ“¦ Parquet: {OUTPUT_PARQUET_PATH}\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Run the main ETL notebook\")\n",
    "print(f\"  2. The pipeline will read these files as source data\")\n",
    "print(f\"  3. Data quality issues will be cleaned during transformation\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "functions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
