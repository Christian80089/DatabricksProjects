{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "32b3971f-dbd2-4de0-a634-3138b3fc58c5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initiate ETL Process from Raw Data to Unity Catalog"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# ETL PIPELINE: RAW DATA TO UNITY CATALOG\n",
    "# ============================================================================\n",
    "# Simple ETL demonstrating: Extract → Transform → Load pattern\n",
    "# Author: Portfolio Project\n",
    "# Last Updated: 2026-02-02\n",
    "# ============================================================================\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, to_date, year, month, round, current_timestamp\n",
    ")\n",
    "from datetime import datetime\n",
    "\n",
    "start_time = datetime.now()\n",
    "print(f\"ETL Pipeline Started: {start_time.strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"Spark Version: {spark.version}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d82abb4-a864-4c18-ae60-5e2df8bdc7d9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Initialize Parameters for Portfolio ETL Configuration"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. PARAMETERS\n",
    "# ============================================================================\n",
    "\n",
    "# Define notebook parameters (can be passed from jobs/workflows)\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"workspace\")\n",
    "dbutils.widgets.text(\"SCHEMA_NAME\", \"portfolio_project\")\n",
    "dbutils.widgets.text(\"TABLE_NAME\", \"sales_clean\")\n",
    "\n",
    "# Get parameter values\n",
    "CATALOG_NAME = dbutils.widgets.get(\"CATALOG_NAME\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"SCHEMA_NAME\")\n",
    "TABLE_NAME = dbutils.widgets.get(\"TABLE_NAME\")\n",
    "FULL_TABLE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# Data paths\n",
    "RAW_CSV_PATH = \"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/\"\n",
    "\n",
    "print(\" Configuration:\")\n",
    "print(f\"  Catalog: {CATALOG_NAME}\")\n",
    "print(f\"  Schema: {SCHEMA_NAME}\")\n",
    "print(f\"  Table: {TABLE_NAME}\")\n",
    "print(f\"  Source: {RAW_CSV_PATH}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a47e9d2a-d2b5-43d8-b870-0703a1a05883",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Setup and Create Portfolio ETL Project Schema"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. SETUP - CREATE SCHEMA\n",
    "# ============================================================================\n",
    "\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\n",
    "    COMMENT 'Portfolio ETL project schema'\n",
    "\"\"\")\n",
    "print(f\"✓ Schema ready: {CATALOG_NAME}.{SCHEMA_NAME}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "146f0b51-b311-49c8-ae74-0bef8dc73fe6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Raw CSV Data into Spark DataFrame for Processing"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. EXTRACT - READ RAW DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"EXTRACT: Loading raw data...\")\n",
    "\n",
    "df_raw = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(RAW_CSV_PATH)\n",
    "\n",
    "raw_count = df_raw.count()\n",
    "print(f\"✓ Loaded {raw_count:,} rows with {len(df_raw.columns)} columns\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "75636c87-8ae6-4cf7-ad8f-9de95fa833f9",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cleaning, Enriching, and Validatio ..."
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 4. TRANSFORM - CLEAN AND ENRICH DATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"TRANSFORM: Cleaning and enriching data...\")\n",
    "\n",
    "# Remove duplicates\n",
    "df_clean = df_raw.dropDuplicates()\n",
    "duplicates_removed = raw_count - df_clean.count()\n",
    "\n",
    "# Handle missing values\n",
    "df_clean = df_clean.na.fill({\n",
    "    \"quantity\": 0,\n",
    "    \"unit_price\": 0.0,\n",
    "    \"discount\": 0.0,\n",
    "    \"category\": \"Unknown\",\n",
    "    \"region\": \"Unspecified\"\n",
    "})\n",
    "\n",
    "# Remove rows with null critical fields\n",
    "df_clean = df_clean \\\n",
    "    .filter(col(\"transaction_id\").isNotNull()) \\\n",
    "    .filter(col(\"transaction_date\").isNotNull()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "\n",
    "# Standardize text fields\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"customer_name\", trim(upper(col(\"customer_name\")))) \\\n",
    "    .withColumn(\"product_name\", trim(upper(col(\"product_name\")))) \\\n",
    "    .withColumn(\"category\", trim(upper(col(\"category\")))) \\\n",
    "    .withColumn(\"region\", trim(upper(col(\"region\")))) \\\n",
    "    .withColumn(\"sales_person\", trim(upper(col(\"sales_person\"))))\n",
    "\n",
    "# Convert date and add derived columns\n",
    "df_clean = df_clean \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"total_amount\", round((col(\"quantity\") * col(\"unit_price\")) * (1 - col(\"discount\")), 2))\n",
    "\n",
    "# Data validation\n",
    "df_clean = df_clean \\\n",
    "    .filter(col(\"quantity\") >= 0) \\\n",
    "    .filter(col(\"unit_price\") >= 0) \\\n",
    "    .filter(col(\"discount\").between(0, 1)) \\\n",
    "    .filter(col(\"quantity\") <= 1000)\n",
    "\n",
    "clean_count = df_clean.count()\n",
    "rows_removed = raw_count - clean_count\n",
    "\n",
    "print(f\"✓ Duplicates removed: {duplicates_removed:,}\")\n",
    "print(f\"✓ Invalid records removed: {rows_removed - duplicates_removed:,}\")\n",
    "print(f\"✓ Clean records: {clean_count:,} ({clean_count/raw_count*100:.1f}% retention)\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca1701e8-8c01-4c83-b518-e5297b098b49",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Load Data to Unity Catalog with Partition Management"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 5. LOAD - WRITE TO UNITY CATALOG\n",
    "# ============================================================================\n",
    "\n",
    "print(\"LOAD: Writing to Unity Catalog...\")\n",
    "\n",
    "# Detect partitions\n",
    "partitions = df_clean.select(\"year\", \"month\").distinct().collect()\n",
    "partition_conditions = [f\"(year = {p.year} AND month = {p.month})\" for p in partitions]\n",
    "replace_where = \" OR \".join(partition_conditions)\n",
    "\n",
    "print(f\"  Partitions to write: {len(partitions)}\")\n",
    "\n",
    "# Check if table exists\n",
    "table_exists = spark.catalog.tableExists(FULL_TABLE_PATH)\n",
    "if table_exists:\n",
    "    pre_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {FULL_TABLE_PATH}\").collect()[0].cnt\n",
    "    print(f\"  Existing rows: {pre_count:,}\")\n",
    "else:\n",
    "    pre_count = 0\n",
    "    print(f\"  Creating new table\")\n",
    "\n",
    "# Write data\n",
    "write_start = datetime.now()\n",
    "\n",
    "df_clean.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .option(\"replaceWhere\", replace_where) \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .saveAsTable(FULL_TABLE_PATH)\n",
    "\n",
    "write_duration = (datetime.now() - write_start).total_seconds()\n",
    "\n",
    "# Post-write validation\n",
    "post_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {FULL_TABLE_PATH}\").collect()[0].cnt\n",
    "\n",
    "print(f\"✓ Write completed in {write_duration:.2f}s\")\n",
    "print(f\"✓ Throughput: {clean_count/write_duration:,.0f} rows/sec\")\n",
    "print(f\"✓ Final row count: {post_count:,}\")\n",
    "if table_exists:\n",
    "    print(f\"✓ Net change: {post_count - pre_count:+,} rows\\n\")\n",
    "else:\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ba2f82a-7c6b-4691-b451-58d6a3528f25",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Delta Table Optimization and Z Order by Category Region"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 6. OPTIMIZE TABLE\n",
    "# ============================================================================\n",
    "\n",
    "print(\"OPTIMIZE: Improving query performance...\")\n",
    "\n",
    "# Configure table properties\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {FULL_TABLE_PATH} SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Optimize with Z-ORDER\n",
    "optimize_start = datetime.now()\n",
    "spark.sql(f\"OPTIMIZE {FULL_TABLE_PATH} ZORDER BY (category, region)\")\n",
    "optimize_duration = (datetime.now() - optimize_start).total_seconds()\n",
    "\n",
    "# Compute statistics\n",
    "spark.sql(f\"ANALYZE TABLE {FULL_TABLE_PATH} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "\n",
    "print(f\"✓ OPTIMIZE completed in {optimize_duration:.2f}s\")\n",
    "print(f\"✓ Z-ORDER applied on: category, region\")\n",
    "print(f\"✓ Statistics computed\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e1a7343-f970-4e18-9814-171661854e78",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "ETL Completion Metrics and Delta Table Overview"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 7. FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "end_time = datetime.now()\n",
    "total_duration = (end_time - start_time).total_seconds()\n",
    "minutes, seconds = divmod(total_duration, 60)\n",
    "\n",
    "# Get table details\n",
    "table_details = spark.sql(f\"DESCRIBE DETAIL {FULL_TABLE_PATH}\").collect()[0]\n",
    "table_size_mb = table_details.sizeInBytes / (1024**2) if hasattr(table_details, 'sizeInBytes') else 0\n",
    "num_files = table_details.numFiles if hasattr(table_details, 'numFiles') else 0\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nPipeline Summary:\")\n",
    "print(f\"  • Source records: {raw_count:,}\")\n",
    "print(f\"  • Records loaded: {post_count:,}\")\n",
    "print(f\"  • Data quality: {clean_count/raw_count*100:.1f}% retention\")\n",
    "print(f\"  • Execution time: {int(minutes)}m {seconds:.1f}s\")\n",
    "\n",
    "print(\"\\nTable Details:\")\n",
    "print(f\"  • Location: {FULL_TABLE_PATH}\")\n",
    "print(f\"  • Size: {table_size_mb:.2f} MB ({num_files} files)\")\n",
    "print(f\"  • Format: Delta Lake with Unity Catalog\")\n",
    "\n",
    "print(\"\\nOptimizations:\")\n",
    "print(f\"  ✓ Partitioned by year/month\")\n",
    "print(f\"  ✓ Z-ORDERed by category/region\")\n",
    "print(f\"  ✓ Auto-optimize enabled\")\n",
    "print(f\"  ✓ Statistics computed\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(f\"  • Query: SELECT * FROM {FULL_TABLE_PATH} WHERE year = 2026\")\n",
    "print(f\"  • Dashboard: Create visualizations in Databricks SQL\")\n",
    "print(f\"  • Schedule: Set up automated daily runs\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\" Started: {start_time.strftime('%H:%M:%S')}\")\n",
    "print(f\" Ended: {end_time.strftime('%H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw data to Clean Data",
   "widgets": {
    "CATALOG_NAME": {
     "currentValue": "workspace",
     "nuid": "0ffda096-8649-4a74-b2ef-64a7af5f0ad9",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SCHEMA_NAME": {
     "currentValue": "portfolio_project",
     "nuid": "b732cabf-4b61-4974-bb98-07104f9f4626",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TABLE_NAME": {
     "currentValue": "sales_clean",
     "nuid": "c3e365bb-1895-4a85-9461-4bbe1dc9ddcc",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sales_clean",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sales_clean",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
