{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca8ae018-bc09-4cd8-bd3e-7bbccde97cc6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================================\n",
    "# SECTION 1: NOTEBOOK CONFIGURATION AND SETUP\n",
    "# ============================================================================\n",
    "# This section initializes the notebook environment and sets up necessary\n",
    "# configurations for the ETL pipeline\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, lower, when, count, sum, avg, \n",
    "    max, min, round, to_date, year, month, current_timestamp,\n",
    "    regexp_replace, coalesce, isnan, lit, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType, TimestampType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "# Display notebook configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL PIPELINE - RAW DATA TO UNITY CATALOG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Execution Time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce880dc4-99e3-45c1-b3c0-1037bfebdf33",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 2"
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: UNITY CATALOG CONFIGURATION\n",
    "# ============================================================================\n",
    "# Define the three-level namespace for Unity Catalog: catalog.schema.table\n",
    "# Unity Catalog provides centralized governance for all data assets\n",
    "\n",
    "# Define Notebook Parameters\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"workspace\")\n",
    "dbutils.widgets.text(\"SCHEMA_NAME\", \"portfolio_project\")\n",
    "dbutils.widgets.text(\"TABLE_NAME\", \"sales_summary\")\n",
    "\n",
    "# Define Unity Catalog namespace variables\n",
    "CATALOG_NAME = dbutils.widgets.get(\"CATALOG_NAME\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"SCHEMA_NAME\")\n",
    "TABLE_NAME = dbutils.widgets.get(\"TABLE_NAME\")\n",
    "FULL_TABLE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# Raw data paths\n",
    "RAW_CSV_PATH = \"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/\"\n",
    "RAW_PARQUET_PATH = \"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/\"\n",
    "\n",
    "print(f\"Unity Catalog Configuration:\")\n",
    "print(f\"  - Catalog: {CATALOG_NAME}\")\n",
    "print(f\"  - Schema: {SCHEMA_NAME}\")\n",
    "print(f\"  - Table: {TABLE_NAME}\")\n",
    "print(f\"  - Full Path: {FULL_TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0846c7f3-9653-4df1-af90-51e443a06cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: CREATE CATALOG AND SCHEMA\n",
    "# ============================================================================\n",
    "# Set up the Unity Catalog infrastructure if it doesn't exist\n",
    "# This ensures proper organization and governance of data assets\n",
    "\n",
    "# Activate the target catalog\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "print(f\"âœ“ Using catalog: {CATALOG_NAME}\")\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\n",
    "    COMMENT 'Portfolio project schema for ETL demonstration'\n",
    "\"\"\")\n",
    "print(f\"âœ“ Schema created/verified: {SCHEMA_NAME}\")\n",
    "\n",
    "# Verify the schema was created successfully\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG_NAME}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d663aa94-60fd-4ff0-90de-981d3f30bd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: EXTRACT - LOAD RAW DATA\n",
    "# ============================================================================\n",
    "# Extract raw data from various file formats\n",
    "# Databricks supports CSV, Parquet, JSON, Delta, and more\n",
    "\n",
    "# Option 1: Load data from CSV\n",
    "print(\"Loading data from CSV...\")\n",
    "df_raw = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(RAW_CSV_PATH)\n",
    "\n",
    "# Option 2: Load data from Parquet (more efficient for large datasets)\n",
    "# Uncomment the following lines to use Parquet instead\n",
    "# print(\"Loading data from Parquet...\")\n",
    "# df_raw = spark.read \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .load(RAW_PARQUET_PATH)\n",
    "\n",
    "# Option 3: Load from Delta Lake (recommended for Databricks)\n",
    "# df_raw = spark.read.format(\"delta\").load(\"/path/to/delta/table\")\n",
    "\n",
    "print(f\"âœ“ Raw data loaded successfully\")\n",
    "print(f\"  - Total rows: {df_raw.count():,}\")\n",
    "print(f\"  - Total columns: {len(df_raw.columns)}\")\n",
    "\n",
    "# Display sample of raw data\n",
    "print(\"\\nSample of raw data (first 10 rows):\")\n",
    "display(df_raw.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae4910a0-8de6-43db-9f33-2ef20455c384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DATA PROFILING AND QUALITY ASSESSMENT\n",
    "# ============================================================================\n",
    "# Before cleaning, it's crucial to understand data quality issues\n",
    "# This section provides comprehensive data profiling\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for null values in each column\n",
    "print(\"\\n1. NULL VALUE ANALYSIS:\")\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "display(null_counts)\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicate_count = df_raw.count() - df_raw.dropDuplicates().count()\n",
    "print(f\"\\n2. DUPLICATE RECORDS: {duplicate_count:,}\")\n",
    "\n",
    "# Basic statistical summary\n",
    "print(\"\\n3. STATISTICAL SUMMARY:\")\n",
    "display(df_raw.describe())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n4. CURRENT DATA TYPES:\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "988892d7-aaf4-4a84-bf94-a2c6e2e5cd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TRANSFORM - DATA CLEANING\n",
    "# ============================================================================\n",
    "# Apply comprehensive data cleaning operations\n",
    "# Each step is explained and can be customized based on your data\n",
    "\n",
    "print(\"Starting data cleaning process...\")\n",
    "\n",
    "# Step 1: Remove exact duplicate rows\n",
    "df_cleaned = df_raw.dropDuplicates()\n",
    "print(f\"âœ“ Step 1: Removed {df_raw.count() - df_cleaned.count()} duplicate rows\")\n",
    "\n",
    "# Step 2: Handle missing values with business logic\n",
    "# Different strategies for different columns\n",
    "df_cleaned = df_cleaned \\\n",
    "    .na.fill({\n",
    "        \"quantity\": 0,          # Fill missing quantities with 0\n",
    "        \"unit_price\": 0.0,      # Fill missing prices with 0\n",
    "        \"discount\": 0.0,        # Assume no discount if missing\n",
    "        \"category\": \"Unknown\",  # Default category\n",
    "        \"region\": \"Unspecified\" # Default region\n",
    "    })\n",
    "print(\"âœ“ Step 2: Filled missing values with appropriate defaults\")\n",
    "\n",
    "# Step 3: Drop rows where critical fields are null\n",
    "# Transaction ID and Date are mandatory for our analysis\n",
    "df_cleaned = df_cleaned \\\n",
    "    .filter(col(\"transaction_id\").isNotNull()) \\\n",
    "    .filter(col(\"transaction_date\").isNotNull()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "print(\"âœ“ Step 3: Removed rows with null critical fields\")\n",
    "\n",
    "# Step 4: Standardize text fields (trim whitespace, consistent casing)\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"customer_name\", trim(upper(col(\"customer_name\")))) \\\n",
    "    .withColumn(\"product_name\", trim(upper(col(\"product_name\")))) \\\n",
    "    .withColumn(\"category\", trim(upper(col(\"category\")))) \\\n",
    "    .withColumn(\"region\", trim(upper(col(\"region\")))) \\\n",
    "    .withColumn(\"sales_person\", trim(upper(col(\"sales_person\"))))\n",
    "print(\"âœ“ Step 4: Standardized text fields (uppercase and trimmed)\")\n",
    "\n",
    "# Step 5: Convert date strings to proper date type\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "print(\"âœ“ Step 5: Converted transaction_date to proper DateType\")\n",
    "\n",
    "# Step 6: Add derived columns for better analysis\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"total_amount\", \n",
    "                round((col(\"quantity\") * col(\"unit_price\")) * (1 - col(\"discount\")), 2))\n",
    "print(\"âœ“ Step 6: Added derived columns (year, month, total_amount)\")\n",
    "\n",
    "# Step 7: Data validation - remove invalid records\n",
    "# Ensure quantity and price are positive\n",
    "df_cleaned = df_cleaned \\\n",
    "    .filter(col(\"quantity\") >= 0) \\\n",
    "    .filter(col(\"unit_price\") >= 0) \\\n",
    "    .filter(col(\"discount\") >= 0) \\\n",
    "    .filter(col(\"discount\") <= 1)  # Discount should be between 0 and 1\n",
    "print(\"âœ“ Step 7: Filtered out invalid records (negative values, invalid discounts)\")\n",
    "\n",
    "# Step 8: Remove outliers (optional - adjust thresholds based on your data)\n",
    "# For example, remove transactions with unrealistic quantities\n",
    "df_cleaned = df_cleaned.filter(col(\"quantity\") <= 1000)\n",
    "print(\"âœ“ Step 8: Removed outliers (quantity > 1000)\")\n",
    "\n",
    "print(f\"\\nCleaning complete:\")\n",
    "print(f\"  - Original rows: {df_raw.count():,}\")\n",
    "print(f\"  - Cleaned rows: {df_cleaned.count():,}\")\n",
    "print(f\"  - Rows removed: {df_raw.count() - df_cleaned.count():,}\")\n",
    "print(f\"  - Data retention rate: {(df_cleaned.count() / df_raw.count() * 100):.2f}%\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df_cleaned.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1313499f-525c-42a4-a789-dd035f58fee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: TRANSFORM - DATA AGGREGATION\n",
    "# ============================================================================\n",
    "# Create aggregated views of the data for analytical purposes\n",
    "# This demonstrates various aggregation techniques in PySpark\n",
    "\n",
    "print(\"Creating aggregated dataset...\")\n",
    "\n",
    "# Aggregation 1: Monthly sales summary by category and region\n",
    "df_aggregated = df_cleaned.groupBy(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"category\",\n",
    "    \"region\"\n",
    ").agg(\n",
    "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "    max(\"total_amount\").alias(\"max_transaction_value\"),\n",
    "    min(\"total_amount\").alias(\"min_transaction_value\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    countDistinct(\"product_id\").alias(\"unique_products\")\n",
    ").orderBy(\"year\", \"month\", \"category\", \"region\")\n",
    "\n",
    "# Round decimal values for better readability\n",
    "df_aggregated = df_aggregated \\\n",
    "    .withColumn(\"total_revenue\", round(col(\"total_revenue\"), 2)) \\\n",
    "    .withColumn(\"avg_transaction_value\", round(col(\"avg_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"max_transaction_value\", round(col(\"max_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"min_transaction_value\", round(col(\"min_transaction_value\"), 2))\n",
    "\n",
    "# Add metadata columns for tracking\n",
    "df_aggregated = df_aggregated \\\n",
    "    .withColumn(\"created_at\", current_timestamp()) \\\n",
    "    .withColumn(\"data_source\", lit(\"sales_etl_pipeline\"))\n",
    "\n",
    "print(f\"âœ“ Aggregation complete\")\n",
    "print(f\"  - Aggregated rows: {df_aggregated.count():,}\")\n",
    "print(f\"  - Aggregation dimensions: year, month, category, region\")\n",
    "print(f\"  - Metrics calculated: 8 business metrics\")\n",
    "\n",
    "# Display aggregated data\n",
    "print(\"\\nSample of aggregated data:\")\n",
    "display(df_aggregated.limit(20))\n",
    "\n",
    "# Show aggregation summary\n",
    "print(\"\\nAggregation summary by category:\")\n",
    "display(\n",
    "    df_aggregated.groupBy(\"category\")\n",
    "    .agg(\n",
    "        sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "        sum(\"total_transactions\").alias(\"category_transactions\")\n",
    "    )\n",
    "    .orderBy(col(\"category_revenue\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d18604e-552c-433b-8b6c-9e6e7a1a7bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: LOAD - WRITE TO UNITY CATALOG\n",
    "# ============================================================================\n",
    "# Save the cleaned and aggregated data to Unity Catalog\n",
    "# Using Delta Lake format for ACID transactions and time travel\n",
    "\n",
    "print(f\"Writing data to Unity Catalog...\")\n",
    "print(f\"Target: {FULL_TABLE_PATH}\")\n",
    "\n",
    "# Write the aggregated data to Unity Catalog as a managed table\n",
    "# Mode options: \"overwrite\", \"append\", \"error\", \"ignore\"\n",
    "df_aggregated.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .saveAsTable(FULL_TABLE_PATH)\n",
    "\n",
    "print(f\"âœ“ Data successfully written to Unity Catalog\")\n",
    "\n",
    "# Add table comment and properties\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {FULL_TABLE_PATH} IS \n",
    "    'Aggregated sales data by month, category, and region. \n",
    "    Source: ETL pipeline from raw sales data. \n",
    "    Updated: {datetime.now()}'\n",
    "\"\"\")\n",
    "\n",
    "# Optimize the table for better query performance\n",
    "spark.sql(f\"OPTIMIZE {FULL_TABLE_PATH}\")\n",
    "print(\"âœ“ Table optimized for query performance\")\n",
    "\n",
    "# Analyze table statistics for the query optimizer\n",
    "spark.sql(f\"ANALYZE TABLE {FULL_TABLE_PATH} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "print(\"âœ“ Table statistics computed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9bd6405a-85f1-47f4-a7b8-91f06b893cda",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 9: DATA VALIDATION AND QUALITY CHECKS\n",
    "# ============================================================================\n",
    "# Verify the data was written correctly and perform quality checks\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"POST-LOAD VALIDATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Verify table exists and count rows\n",
    "table_count = spark.sql(f\"SELECT COUNT(*) as row_count FROM {FULL_TABLE_PATH}\").collect()[0][0]\n",
    "print(f\"\\nâœ“ Table exists with {table_count:,} rows\")\n",
    "\n",
    "# Check 2: Display table schema\n",
    "print(\"\\nTable schema:\")\n",
    "spark.sql(f\"DESCRIBE TABLE {FULL_TABLE_PATH}\").show(truncate=False)\n",
    "\n",
    "# Check 3: Display sample data from Unity Catalog\n",
    "print(\"\\nSample data from Unity Catalog table:\")\n",
    "display(spark.sql(f\"SELECT * FROM {FULL_TABLE_PATH} LIMIT 10\"))\n",
    "\n",
    "# Check 4: Run basic analytics query\n",
    "print(\"\\nTop 5 categories by revenue:\")\n",
    "top_categories = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        SUM(total_revenue) as total_revenue,\n",
    "        SUM(total_transactions) as total_transactions,\n",
    "        SUM(unique_customers) as total_customers\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(top_categories)\n",
    "\n",
    "# Check 5: Verify data integrity\n",
    "print(\"\\nData integrity checks:\")\n",
    "integrity_check = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_rows,\n",
    "        COUNT(DISTINCT year, month, category, region) as unique_combinations,\n",
    "        SUM(CASE WHEN total_revenue < 0 THEN 1 ELSE 0 END) as negative_revenue_count,\n",
    "        SUM(CASE WHEN total_transactions < 0 THEN 1 ELSE 0 END) as negative_transaction_count\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "\"\"\")\n",
    "display(integrity_check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81dd8ae-5806-4006-b79b-595a09eedd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸ“Š Summary:\")\n",
    "print(f\"  â€¢ Source: Raw CSV/Parquet files\")\n",
    "print(f\"  â€¢ Destination: {FULL_TABLE_PATH}\")\n",
    "print(f\"  â€¢ Records processed: {df_raw.count():,}\")\n",
    "print(f\"  â€¢ Records loaded: {table_count:,}\")\n",
    "print(f\"  â€¢ Data quality: âœ“ Cleaned and validated\")\n",
    "print(f\"  â€¢ Format: Delta Lake\")\n",
    "print(f\"  â€¢ Governance: Unity Catalog\")\n",
    "print(f\"\\nâœ… Data is now available for analytics and reporting\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Query the table: SELECT * FROM {FULL_TABLE_PATH}\")\n",
    "print(f\"  2. Create dashboards using Databricks SQL\")\n",
    "print(f\"  3. Set up automated jobs for regular updates\")\n",
    "print(f\"  4. Configure alerts for data quality monitoring\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw data to Aggregate Data",
   "widgets": {
    "": {
     "currentValue": "sales_summary",
     "nuid": "bdd260b0-0ecb-4d3a-ac13-5e915355666d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sales_summary",
      "label": null,
      "name": "",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sales_summary",
      "label": null,
      "name": "",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "CATALOG_NAME": {
     "currentValue": "workspace",
     "nuid": "f22e09ea-d33e-4450-a713-38ead9479412",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SCHEMA_NAME": {
     "currentValue": "portfolio_projects",
     "nuid": "4a8b1146-f741-4187-ad81-e54b1599f703",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TABLE_NAME": {
     "currentValue": "sales_summary",
     "nuid": "7d606ef1-28d4-4467-b99e-812490b6a637",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sales_summary",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sales_summary",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
