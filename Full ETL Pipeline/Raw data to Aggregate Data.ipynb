{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ca8ae018-bc09-4cd8-bd3e-7bbccde97cc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================================\n",
    "# SECTION 1: NOTEBOOK CONFIGURATION AND SETUP\n",
    "# ============================================================================\n",
    "# This section initializes the notebook environment and sets up necessary\n",
    "# configurations for the ETL pipeline\n",
    "\n",
    "# Import required libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import (\n",
    "    col, trim, upper, lower, when, count, sum, avg, \n",
    "    max, min, round, to_date, year, month, current_timestamp,\n",
    "    regexp_replace, coalesce, isnan, lit, countDistinct\n",
    ")\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, IntegerType, \n",
    "    DoubleType, DateType, TimestampType\n",
    ")\n",
    "from pyspark.sql.window import Window\n",
    "from datetime import datetime\n",
    "\n",
    "start_notebook_datetime = datetime.now()\n",
    "\n",
    "# Display notebook configuration\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL PIPELINE - RAW DATA TO UNITY CATALOG\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Start Execution Time: {start_notebook_datetime}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ce880dc4-99e3-45c1-b3c0-1037bfebdf33",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 2: UNITY CATALOG CONFIGURATION\n",
    "# ============================================================================\n",
    "# Define the three-level namespace for Unity Catalog: catalog.schema.table\n",
    "# Unity Catalog provides centralized governance for all data assets\n",
    "\n",
    "# Define Notebook Parameters\n",
    "dbutils.widgets.text(\"CATALOG_NAME\", \"workspace\")\n",
    "dbutils.widgets.text(\"SCHEMA_NAME\", \"portfolio_project\")\n",
    "dbutils.widgets.text(\"TABLE_NAME\", \"sales_summary\")\n",
    "\n",
    "# Define Unity Catalog namespace variables\n",
    "CATALOG_NAME = dbutils.widgets.get(\"CATALOG_NAME\")\n",
    "SCHEMA_NAME = dbutils.widgets.get(\"SCHEMA_NAME\")\n",
    "TABLE_NAME = dbutils.widgets.get(\"TABLE_NAME\")\n",
    "FULL_TABLE_PATH = f\"{CATALOG_NAME}.{SCHEMA_NAME}.{TABLE_NAME}\"\n",
    "\n",
    "# Raw data paths\n",
    "RAW_CSV_PATH = \"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_csv/\"\n",
    "RAW_PARQUET_PATH = \"/Volumes/workspace/portfolio_projects/volume_portfolio_projects/simple_etl_project_raw_data_parquet/\"\n",
    "\n",
    "print(f\"Unity Catalog Configuration:\")\n",
    "print(f\"  - Catalog: {CATALOG_NAME}\")\n",
    "print(f\"  - Schema: {SCHEMA_NAME}\")\n",
    "print(f\"  - Table: {TABLE_NAME}\")\n",
    "print(f\"  - Full Path: {FULL_TABLE_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "0846c7f3-9653-4df1-af90-51e443a06cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 3: CREATE CATALOG AND SCHEMA\n",
    "# ============================================================================\n",
    "# Set up the Unity Catalog infrastructure if it doesn't exist\n",
    "# This ensures proper organization and governance of data assets\n",
    "\n",
    "# Activate the target catalog\n",
    "spark.sql(f\"USE CATALOG {CATALOG_NAME}\")\n",
    "print(f\"‚úì Using catalog: {CATALOG_NAME}\")\n",
    "\n",
    "# Create schema if it doesn't exist\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE SCHEMA IF NOT EXISTS {CATALOG_NAME}.{SCHEMA_NAME}\n",
    "    COMMENT 'Portfolio project schema for ETL demonstration'\n",
    "\"\"\")\n",
    "print(f\"‚úì Schema created/verified: {SCHEMA_NAME}\")\n",
    "\n",
    "# Verify the schema was created successfully\n",
    "schemas_df = spark.sql(f\"SHOW SCHEMAS IN {CATALOG_NAME}\")\n",
    "display(schemas_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "d663aa94-60fd-4ff0-90de-981d3f30bd4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 4: EXTRACT - LOAD RAW DATA\n",
    "# ============================================================================\n",
    "# Extract raw data from various file formats\n",
    "# Databricks supports CSV, Parquet, JSON, Delta, and more\n",
    "\n",
    "# Option 1: Load data from CSV\n",
    "print(\"Loading data from CSV...\")\n",
    "df_raw = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"dateFormat\", \"yyyy-MM-dd\") \\\n",
    "    .option(\"nullValue\", \"NULL\") \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .load(RAW_CSV_PATH)\n",
    "\n",
    "# Option 2: Load data from Parquet (more efficient for large datasets)\n",
    "# Uncomment the following lines to use Parquet instead\n",
    "# print(\"Loading data from Parquet...\")\n",
    "# df_raw = spark.read \\\n",
    "#     .format(\"parquet\") \\\n",
    "#     .load(RAW_PARQUET_PATH)\n",
    "\n",
    "# Option 3: Load from Delta Lake (recommended for Databricks)\n",
    "# df_raw = spark.read.format(\"delta\").load(\"/path/to/delta/table\")\n",
    "\n",
    "print(f\"‚úì Raw data loaded successfully\")\n",
    "print(f\"  - Total rows: {df_raw.count():,}\")\n",
    "print(f\"  - Total columns: {len(df_raw.columns)}\")\n",
    "\n",
    "# Display sample of raw data\n",
    "print(\"\\nSample of raw data (first 10 rows):\")\n",
    "display(df_raw.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "ae4910a0-8de6-43db-9f33-2ef20455c384",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 5: DATA PROFILING AND QUALITY ASSESSMENT\n",
    "# ============================================================================\n",
    "# Before cleaning, it's crucial to understand data quality issues\n",
    "# This section provides comprehensive data profiling\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATA QUALITY ASSESSMENT\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check for null values in each column\n",
    "print(\"\\n1. NULL VALUE ANALYSIS:\")\n",
    "null_counts = df_raw.select([\n",
    "    count(when(col(c).isNull(), c)).alias(c) \n",
    "    for c in df_raw.columns\n",
    "])\n",
    "display(null_counts)\n",
    "\n",
    "# Check for duplicate records\n",
    "duplicate_count = df_raw.count() - df_raw.dropDuplicates().count()\n",
    "print(f\"\\n2. DUPLICATE RECORDS: {duplicate_count:,}\")\n",
    "\n",
    "# Basic statistical summary\n",
    "print(\"\\n3. STATISTICAL SUMMARY:\")\n",
    "display(df_raw.describe())\n",
    "\n",
    "# Check data types\n",
    "print(\"\\n4. CURRENT DATA TYPES:\")\n",
    "df_raw.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "988892d7-aaf4-4a84-bf94-a2c6e2e5cd6f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 6: TRANSFORM - DATA CLEANING\n",
    "# ============================================================================\n",
    "# Apply comprehensive data cleaning operations\n",
    "# Each step is explained and can be customized based on your data\n",
    "\n",
    "print(\"Starting data cleaning process...\")\n",
    "\n",
    "# Step 1: Remove exact duplicate rows\n",
    "df_cleaned = df_raw.dropDuplicates()\n",
    "print(f\"‚úì Step 1: Removed {df_raw.count() - df_cleaned.count()} duplicate rows\")\n",
    "\n",
    "# Step 2: Handle missing values with business logic\n",
    "# Different strategies for different columns\n",
    "df_cleaned = df_cleaned \\\n",
    "    .na.fill({\n",
    "        \"quantity\": 0,          # Fill missing quantities with 0\n",
    "        \"unit_price\": 0.0,      # Fill missing prices with 0\n",
    "        \"discount\": 0.0,        # Assume no discount if missing\n",
    "        \"category\": \"Unknown\",  # Default category\n",
    "        \"region\": \"Unspecified\" # Default region\n",
    "    })\n",
    "print(\"‚úì Step 2: Filled missing values with appropriate defaults\")\n",
    "\n",
    "# Step 3: Drop rows where critical fields are null\n",
    "# Transaction ID and Date are mandatory for our analysis\n",
    "df_cleaned = df_cleaned \\\n",
    "    .filter(col(\"transaction_id\").isNotNull()) \\\n",
    "    .filter(col(\"transaction_date\").isNotNull()) \\\n",
    "    .filter(col(\"customer_id\").isNotNull())\n",
    "print(\"‚úì Step 3: Removed rows with null critical fields\")\n",
    "\n",
    "# Step 4: Standardize text fields (trim whitespace, consistent casing)\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"customer_name\", trim(upper(col(\"customer_name\")))) \\\n",
    "    .withColumn(\"product_name\", trim(upper(col(\"product_name\")))) \\\n",
    "    .withColumn(\"category\", trim(upper(col(\"category\")))) \\\n",
    "    .withColumn(\"region\", trim(upper(col(\"region\")))) \\\n",
    "    .withColumn(\"sales_person\", trim(upper(col(\"sales_person\"))))\n",
    "print(\"‚úì Step 4: Standardized text fields (uppercase and trimmed)\")\n",
    "\n",
    "# Step 5: Convert date strings to proper date type\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"transaction_date\", to_date(col(\"transaction_date\"), \"yyyy-MM-dd\"))\n",
    "print(\"‚úì Step 5: Converted transaction_date to proper DateType\")\n",
    "\n",
    "# Step 6: Add derived columns for better analysis\n",
    "df_cleaned = df_cleaned \\\n",
    "    .withColumn(\"year\", year(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"month\", month(col(\"transaction_date\"))) \\\n",
    "    .withColumn(\"total_amount\", \n",
    "                round((col(\"quantity\") * col(\"unit_price\")) * (1 - col(\"discount\")), 2))\n",
    "print(\"‚úì Step 6: Added derived columns (year, month, total_amount)\")\n",
    "\n",
    "# Step 7: Data validation - remove invalid records\n",
    "# Ensure quantity and price are positive\n",
    "df_cleaned = df_cleaned \\\n",
    "    .filter(col(\"quantity\") >= 0) \\\n",
    "    .filter(col(\"unit_price\") >= 0) \\\n",
    "    .filter(col(\"discount\") >= 0) \\\n",
    "    .filter(col(\"discount\") <= 1)  # Discount should be between 0 and 1\n",
    "print(\"‚úì Step 7: Filtered out invalid records (negative values, invalid discounts)\")\n",
    "\n",
    "# Step 8: Remove outliers (optional - adjust thresholds based on your data)\n",
    "# For example, remove transactions with unrealistic quantities\n",
    "df_cleaned = df_cleaned.filter(col(\"quantity\") <= 1000)\n",
    "print(\"‚úì Step 8: Removed outliers (quantity > 1000)\")\n",
    "\n",
    "print(f\"\\nCleaning complete:\")\n",
    "print(f\"  - Original rows: {df_raw.count():,}\")\n",
    "print(f\"  - Cleaned rows: {df_cleaned.count():,}\")\n",
    "print(f\"  - Rows removed: {df_raw.count() - df_cleaned.count():,}\")\n",
    "print(f\"  - Data retention rate: {(df_cleaned.count() / df_raw.count() * 100):.2f}%\")\n",
    "\n",
    "# Display sample of cleaned data\n",
    "print(\"\\nSample of cleaned data:\")\n",
    "display(df_cleaned.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "1313499f-525c-42a4-a789-dd035f58fee1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 7: TRANSFORM - DATA AGGREGATION\n",
    "# ============================================================================\n",
    "# Create aggregated views of the data for analytical purposes\n",
    "# This demonstrates various aggregation techniques in PySpark\n",
    "\n",
    "print(\"Creating aggregated dataset...\")\n",
    "\n",
    "# Aggregation 1: Monthly sales summary by category and region\n",
    "df_aggregated = df_cleaned.groupBy(\n",
    "    \"year\",\n",
    "    \"month\",\n",
    "    \"category\",\n",
    "    \"region\"\n",
    ").agg(\n",
    "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    sum(\"quantity\").alias(\"total_quantity_sold\"),\n",
    "    sum(\"total_amount\").alias(\"total_revenue\"),\n",
    "    avg(\"total_amount\").alias(\"avg_transaction_value\"),\n",
    "    max(\"total_amount\").alias(\"max_transaction_value\"),\n",
    "    min(\"total_amount\").alias(\"min_transaction_value\"),\n",
    "    countDistinct(\"customer_id\").alias(\"unique_customers\"),\n",
    "    countDistinct(\"product_id\").alias(\"unique_products\")\n",
    ").orderBy(\"year\", \"month\", \"category\", \"region\")\n",
    "\n",
    "# Round decimal values for better readability\n",
    "df_aggregated = df_aggregated \\\n",
    "    .withColumn(\"total_revenue\", round(col(\"total_revenue\"), 2)) \\\n",
    "    .withColumn(\"avg_transaction_value\", round(col(\"avg_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"max_transaction_value\", round(col(\"max_transaction_value\"), 2)) \\\n",
    "    .withColumn(\"min_transaction_value\", round(col(\"min_transaction_value\"), 2))\n",
    "\n",
    "# Add metadata columns for tracking\n",
    "df_aggregated = df_aggregated \\\n",
    "    .withColumn(\"created_at\", current_timestamp()) \\\n",
    "    .withColumn(\"data_source\", lit(\"sales_etl_pipeline\"))\n",
    "\n",
    "print(f\"‚úì Aggregation complete\")\n",
    "print(f\"  - Aggregated rows: {df_aggregated.count():,}\")\n",
    "print(f\"  - Aggregation dimensions: year, month, category, region\")\n",
    "print(f\"  - Metrics calculated: 8 business metrics\")\n",
    "\n",
    "# Display aggregated data\n",
    "print(\"\\nSample of aggregated data:\")\n",
    "display(df_aggregated.limit(20))\n",
    "\n",
    "# Show aggregation summary\n",
    "print(\"\\nAggregation summary by category:\")\n",
    "display(\n",
    "    df_aggregated.groupBy(\"category\")\n",
    "    .agg(\n",
    "        sum(\"total_revenue\").alias(\"category_revenue\"),\n",
    "        sum(\"total_transactions\").alias(\"category_transactions\")\n",
    "    )\n",
    "    .orderBy(col(\"category_revenue\").desc())\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5d18604e-552c-433b-8b6c-9e6e7a1a7bd8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# SECTION 8: LOAD - WRITE TO UNITY CATALOG WITH SMART PARTITIONING\n",
    "# ============================================================================\n",
    "# Save the cleaned and aggregated data to Unity Catalog\n",
    "# Using Delta Lake format with automatic partition detection and selective overwrite\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"WRITING TO UNITY CATALOG - PARTITIONED TABLE\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Target: {FULL_TABLE_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: ANALYZE PARTITIONS IN INPUT DATA\n",
    "# ============================================================================\n",
    "\n",
    "# Automatically detect which partitions are present in the incoming data\n",
    "partitions_df = df_aggregated.select(\"year\", \"month\").distinct().orderBy(\"year\", \"month\")\n",
    "partitions_list = partitions_df.collect()\n",
    "\n",
    "print(f\"\\nüìä Partition Analysis:\")\n",
    "print(f\"  - Total partitions in input data: {len(partitions_list)}\")\n",
    "print(f\"  - Partition columns: year, month\")\n",
    "print(f\"\\n  Partitions to be overwritten:\")\n",
    "\n",
    "# Build the replaceWhere clause dynamically\n",
    "partition_conditions = []\n",
    "for partition in partitions_list:\n",
    "    year_val = partition.year\n",
    "    month_val = partition.month\n",
    "    print(f\"    ‚Ä¢ year={year_val}, month={month_val}\")\n",
    "    partition_conditions.append(f\"(year = {year_val} AND month = {month_val})\")\n",
    "\n",
    "# Combine all partition conditions with OR\n",
    "replace_where_clause = \" OR \".join(partition_conditions)\n",
    "print(f\"\\n  Generated replaceWhere clause:\")\n",
    "print(f\"    {replace_where_clause[:150]}{'...' if len(replace_where_clause) > 150 else ''}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: COLLECT PRE-WRITE STATISTICS (if table exists)\n",
    "# ============================================================================\n",
    "\n",
    "# Check if table already exists to collect statistics\n",
    "table_exists = spark.catalog.tableExists(FULL_TABLE_PATH)\n",
    "\n",
    "if table_exists:\n",
    "    print(f\"\\nüìà Pre-Write Statistics:\")\n",
    "    \n",
    "    # Total row count before\n",
    "    pre_total_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {FULL_TABLE_PATH}\").collect()[0].cnt\n",
    "    print(f\"  - Total rows before: {pre_total_count:,}\")\n",
    "    \n",
    "    # Count rows in partitions that will be overwritten\n",
    "    pre_partition_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt \n",
    "        FROM {FULL_TABLE_PATH} \n",
    "        WHERE {replace_where_clause}\n",
    "    \"\"\").collect()[0].cnt\n",
    "    print(f\"  - Rows in target partitions before: {pre_partition_count:,}\")\n",
    "    \n",
    "    # Partition distribution before\n",
    "    print(f\"\\n  Current partition distribution:\")\n",
    "    pre_partition_dist = spark.sql(f\"\"\"\n",
    "        SELECT year, month, COUNT(*) as row_count\n",
    "        FROM {FULL_TABLE_PATH}\n",
    "        GROUP BY year, month\n",
    "        ORDER BY year DESC, month DESC\n",
    "    \"\"\")\n",
    "    display(pre_partition_dist)\n",
    "    \n",
    "    # Table size estimation\n",
    "    table_details = spark.sql(f\"DESCRIBE DETAIL {FULL_TABLE_PATH}\").collect()[0]\n",
    "    pre_size_bytes = table_details.sizeInBytes if hasattr(table_details, 'sizeInBytes') else 0\n",
    "    pre_num_files = table_details.numFiles if hasattr(table_details, 'numFiles') else 0\n",
    "    print(f\"\\n  Table storage metrics:\")\n",
    "    print(f\"    - Size: {pre_size_bytes / (1024**2):.2f} MB\")\n",
    "    print(f\"    - Number of files: {pre_num_files}\")\n",
    "else:\n",
    "    print(f\"\\nüìä Table does not exist yet - will be created\")\n",
    "    pre_total_count = 0\n",
    "    pre_partition_count = 0\n",
    "    pre_size_bytes = 0\n",
    "    pre_num_files = 0\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: WRITE DATA WITH SELECTIVE PARTITION OVERWRITE\n",
    "# ============================================================================\n",
    "\n",
    "write_start_time = datetime.now()\n",
    "print(f\"\\n‚è≥ Starting write operation at {write_start_time.strftime('%H:%M:%S')}\")\n",
    "\n",
    "# Count rows being written\n",
    "rows_to_write = df_aggregated.count()\n",
    "print(f\"  - Rows to write: {rows_to_write:,}\")\n",
    "\n",
    "# Write with replaceWhere for selective partition overwrite\n",
    "df_aggregated.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"year\", \"month\") \\\n",
    "    .option(\"replaceWhere\", replace_where_clause) \\\n",
    "    .option(\"overwriteSchema\", \"false\") \\\n",
    "    .option(\"delta.columnMapping.mode\", \"name\") \\\n",
    "    .saveAsTable(FULL_TABLE_PATH)\n",
    "\n",
    "write_end_time = datetime.now()\n",
    "write_duration = (write_end_time - write_start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úì Data successfully written to Unity Catalog\")\n",
    "print(f\"  - Write duration: {write_duration:.2f} seconds\")\n",
    "print(f\"  - Throughput: {rows_to_write/write_duration:,.0f} rows/second\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: POST-WRITE VALIDATION AND STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Post-Write Statistics:\")\n",
    "\n",
    "# Total row count after\n",
    "post_total_count = spark.sql(f\"SELECT COUNT(*) as cnt FROM {FULL_TABLE_PATH}\").collect()[0].cnt\n",
    "print(f\"  - Total rows after: {post_total_count:,}\")\n",
    "\n",
    "# Count rows in affected partitions\n",
    "post_partition_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as cnt \n",
    "    FROM {FULL_TABLE_PATH} \n",
    "    WHERE {replace_where_clause}\n",
    "\"\"\").collect()[0].cnt\n",
    "print(f\"  - Rows in target partitions after: {post_partition_count:,}\")\n",
    "\n",
    "# Calculate changes\n",
    "if table_exists:\n",
    "    rows_added = post_total_count - pre_total_count\n",
    "    rows_replaced = pre_partition_count\n",
    "    print(f\"\\n  Changes:\")\n",
    "    print(f\"    ‚Ä¢ Rows in affected partitions replaced: {rows_replaced:,}\")\n",
    "    print(f\"    ‚Ä¢ Net change in total rows: {rows_added:+,}\")\n",
    "    print(f\"    ‚Ä¢ Rows in unaffected partitions: {post_total_count - post_partition_count:,} (unchanged)\")\n",
    "\n",
    "# Partition distribution after\n",
    "print(f\"\\n  Updated partition distribution:\")\n",
    "post_partition_dist = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        year, \n",
    "        month, \n",
    "        COUNT(*) as row_count,\n",
    "        ROUND(SUM(total_revenue), 2) as partition_revenue,\n",
    "        COUNT(DISTINCT category) as distinct_categories,\n",
    "        COUNT(DISTINCT region) as distinct_regions\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "display(post_partition_dist)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: TABLE METADATA AND PROPERTIES\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n‚öôÔ∏è  Configuring table properties...\")\n",
    "\n",
    "# Add table comment with detailed metadata\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {FULL_TABLE_PATH} IS \n",
    "    'Aggregated sales data by month, category, and region.\n",
    "    \n",
    "    Partitioning: year, month\n",
    "    Granularity: Monthly aggregations\n",
    "    Source: ETL pipeline from raw sales data\n",
    "    Last Updated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "    Rows: {post_total_count:,}\n",
    "    Partitions: {len(partitions_list)}'\n",
    "\"\"\")\n",
    "\n",
    "# Enable auto-optimization features\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {FULL_TABLE_PATH} SET TBLPROPERTIES (\n",
    "        'delta.autoOptimize.optimizeWrite' = 'true',\n",
    "        'delta.autoOptimize.autoCompact' = 'true',\n",
    "        'delta.logRetentionDuration' = '30 days',\n",
    "        'delta.deletedFileRetentionDuration' = '7 days'\n",
    "    )\n",
    "\"\"\")\n",
    "print(\"‚úì Auto-optimize and retention policies configured\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: OPTIMIZE TABLE FOR QUERY PERFORMANCE\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüöÄ Optimizing table for query performance...\")\n",
    "\n",
    "optimize_start_time = datetime.now()\n",
    "\n",
    "# Optimize with Z-ORDER on frequently queried columns\n",
    "spark.sql(f\"\"\"\n",
    "    OPTIMIZE {FULL_TABLE_PATH} \n",
    "    ZORDER BY (category, region)\n",
    "\"\"\")\n",
    "\n",
    "optimize_end_time = datetime.now()\n",
    "optimize_duration = (optimize_end_time - optimize_start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úì OPTIMIZE completed in {optimize_duration:.2f} seconds\")\n",
    "print(f\"  - Z-ORDER applied on: category, region\")\n",
    "print(f\"  - Benefit: Faster filtering on these columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: COMPUTE TABLE STATISTICS\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìä Computing statistics for query optimizer...\")\n",
    "\n",
    "stats_start_time = datetime.now()\n",
    "\n",
    "# Compute comprehensive statistics for all columns\n",
    "spark.sql(f\"ANALYZE TABLE {FULL_TABLE_PATH} COMPUTE STATISTICS FOR ALL COLUMNS\")\n",
    "\n",
    "stats_end_time = datetime.now()\n",
    "stats_duration = (stats_end_time - stats_start_time).total_seconds()\n",
    "\n",
    "print(f\"‚úì Statistics computed in {stats_duration:.2f} seconds\")\n",
    "print(f\"  - Benefit: Better query execution plans\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 8: DETAILED TABLE INSIGHTS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"TABLE INSIGHTS AND PERFORMANCE METRICS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Get detailed table information\n",
    "table_details = spark.sql(f\"DESCRIBE DETAIL {FULL_TABLE_PATH}\").collect()[0]\n",
    "\n",
    "post_size_bytes = table_details.sizeInBytes if hasattr(table_details, 'sizeInBytes') else 0\n",
    "post_num_files = table_details.numFiles if hasattr(table_details, 'numFiles') else 0\n",
    "\n",
    "print(f\"\\nüì¶ Storage Metrics:\")\n",
    "print(f\"  - Table size: {post_size_bytes / (1024**2):.2f} MB\")\n",
    "print(f\"  - Number of files: {post_num_files}\")\n",
    "print(f\"  - Average file size: {post_size_bytes / post_num_files / (1024**2):.2f} MB\" if post_num_files > 0 else \"  - Average file size: N/A\")\n",
    "print(f\"  - Format: Delta Lake (Parquet + transaction log)\")\n",
    "\n",
    "if table_exists:\n",
    "    size_change = post_size_bytes - pre_size_bytes\n",
    "    file_change = post_num_files - pre_num_files\n",
    "    print(f\"\\n  Storage changes:\")\n",
    "    print(f\"    ‚Ä¢ Size change: {size_change / (1024**2):+.2f} MB\")\n",
    "    print(f\"    ‚Ä¢ File count change: {file_change:+d}\")\n",
    "\n",
    "# Show partition file distribution\n",
    "print(f\"\\nüìÅ Partition File Distribution:\")\n",
    "partition_details = spark.sql(f\"\"\"\n",
    "    SELECT year, month, COUNT(*) as record_count\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "    GROUP BY year, month\n",
    "    ORDER BY year DESC, month DESC\n",
    "\"\"\")\n",
    "display(partition_details)\n",
    "\n",
    "# Show physical partition paths\n",
    "print(f\"\\nüóÇÔ∏è  Physical Partitions:\")\n",
    "physical_partitions = spark.sql(f\"SHOW PARTITIONS {FULL_TABLE_PATH}\").limit(10)\n",
    "display(physical_partitions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "6e0f503d-fc55-4dee-87ee-56d8f4b3569f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 9: QUERY PERFORMANCE TESTING\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n‚ö° Query Performance Testing:\")\n",
    "\n",
    "# Test 1: Full table scan\n",
    "test1_start = datetime.now()\n",
    "full_scan_count = spark.sql(f\"SELECT COUNT(*) FROM {FULL_TABLE_PATH}\").collect()[0][0]\n",
    "test1_duration = (datetime.now() - test1_start).total_seconds()\n",
    "print(f\"  Test 1 - Full table scan: {test1_duration:.3f}s ({full_scan_count:,} rows)\")\n",
    "\n",
    "# Test 2: Single partition query (partition pruning)\n",
    "if len(partitions_list) > 0:\n",
    "    test_year = partitions_list[0].year\n",
    "    test_month = partitions_list[0].month\n",
    "    test2_start = datetime.now()\n",
    "    partition_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) FROM {FULL_TABLE_PATH} \n",
    "        WHERE year = {test_year} AND month = {test_month}\n",
    "    \"\"\").collect()[0][0]\n",
    "    test2_duration = (datetime.now() - test2_start).total_seconds()\n",
    "    print(f\"  Test 2 - Single partition (year={test_year}, month={test_month}): {test2_duration:.3f}s ({partition_count:,} rows)\")\n",
    "    \n",
    "    if test2_duration > 0:\n",
    "        print(f\"    ‚Üí Speedup from partition pruning: {test1_duration/test2_duration:.1f}x faster\")\n",
    "\n",
    "# Test 3: Z-ORDER column filter\n",
    "test3_start = datetime.now()\n",
    "category_count = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) FROM {FULL_TABLE_PATH} \n",
    "    WHERE category = 'ELECTRONICS'\n",
    "\"\"\").collect()[0][0]\n",
    "test3_duration = (datetime.now() - test3_start).total_seconds()\n",
    "print(f\"  Test 3 - Category filter (Z-ORDERed): {test3_duration:.3f}s ({category_count:,} rows)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2da8f653-ee3b-49e8-b91e-3b426283766b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 10: TABLE HISTORY AND VERSION CONTROL\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\nüìú Table History (Delta Time Travel):\")\n",
    "\n",
    "# Show last 5 operations\n",
    "history_df = spark.sql(f\"DESCRIBE HISTORY {FULL_TABLE_PATH}\").limit(5)\n",
    "display(history_df.select(\n",
    "    \"version\", \n",
    "    \"timestamp\", \n",
    "    \"operation\", \n",
    "    \"operationMetrics\",\n",
    "    \"userName\"\n",
    "))\n",
    "\n",
    "# Get current version\n",
    "current_version = spark.sql(f\"DESCRIBE HISTORY {FULL_TABLE_PATH}\").first().version\n",
    "print(f\"\\n  Current table version: {current_version}\")\n",
    "print(f\"  You can time travel to any previous version using:\")\n",
    "print(f\"    SELECT * FROM {FULL_TABLE_PATH} VERSION AS OF <version>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "40b83b3e-83fb-4829-ad0e-3cf0a7b3fe67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# STEP 11: DATA QUALITY VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(f\"\\n‚úÖ Data Quality Checks:\")\n",
    "\n",
    "# Check 1: No null values in partition columns\n",
    "null_partitions = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as null_count\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "    WHERE year IS NULL OR month IS NULL\n",
    "\"\"\").collect()[0].null_count\n",
    "\n",
    "print(f\"  ‚úì Null partition keys: {null_partitions} (should be 0)\")\n",
    "\n",
    "# Check 2: Validate aggregation logic\n",
    "print(f\"\\n  Business metrics validation:\")\n",
    "validation_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT year) as distinct_years,\n",
    "        COUNT(DISTINCT month) as distinct_months,\n",
    "        COUNT(DISTINCT category) as distinct_categories,\n",
    "        COUNT(DISTINCT region) as distinct_regions,\n",
    "        MIN(total_transactions) as min_transactions,\n",
    "        MAX(total_transactions) as max_transactions,\n",
    "        ROUND(SUM(total_revenue), 2) as grand_total_revenue\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "\"\"\")\n",
    "display(validation_df)\n",
    "\n",
    "# Check 3: Top performers\n",
    "print(f\"\\n  Top 5 performing segments:\")\n",
    "top_performers = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        year, month, category, region,\n",
    "        ROUND(total_revenue, 2) as revenue,\n",
    "        total_transactions as transactions\n",
    "    FROM {FULL_TABLE_PATH}\n",
    "    ORDER BY total_revenue DESC\n",
    "    LIMIT 5\n",
    "\"\"\")\n",
    "display(top_performers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fef264da-0484-4747-8913-f612c34bd2a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Databricks notebook source\n",
    "# ============================================================================\n",
    "# SECTION 12: ADD TAGS, COMMENTS AND METADATA TO TABLE AND COLUMNS\n",
    "# ============================================================================\n",
    "# Enhance data discoverability and governance by adding comprehensive metadata\n",
    "# Tags and comments help data consumers understand the data and its usage\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ADDING METADATA: TAGS, COMMENTS AND DESCRIPTIONS\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Target: {FULL_TABLE_PATH}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 1: ADD COLUMN COMMENTS (DESCRIPTIONS)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìù Adding column descriptions...\")\n",
    "\n",
    "# Define column comments for data documentation\n",
    "# These descriptions appear in Catalog Explorer and DESCRIBE commands\n",
    "column_comments = {\n",
    "    \"year\": \"Transaction year - Used for partitioning\",\n",
    "    \"month\": \"Transaction month (1-12) - Used for partitioning\",\n",
    "    \"category\": \"Product category - Z-ORDERed for query optimization\",\n",
    "    \"region\": \"Geographic region where the sale occurred - Z-ORDERed for query optimization\",\n",
    "    \"total_transactions\": \"Total number of individual transactions in this segment\",\n",
    "    \"total_quantity_sold\": \"Sum of all quantities sold across all transactions\",\n",
    "    \"total_revenue\": \"Total revenue generated (quantity * unit_price * (1-discount))\",\n",
    "    \"avg_transaction_value\": \"Average revenue per transaction in this segment\",\n",
    "    \"max_transaction_value\": \"Maximum single transaction value in this segment\",\n",
    "    \"min_transaction_value\": \"Minimum single transaction value in this segment\",\n",
    "    \"unique_customers\": \"Count of distinct customers who made purchases\",\n",
    "    \"unique_products\": \"Count of distinct products sold in this segment\",\n",
    "    \"created_at\": \"Timestamp when this record was created in the table\",\n",
    "    \"data_source\": \"Source system or pipeline that created this record\"\n",
    "}\n",
    "\n",
    "# Apply comments to all columns at once (supports multiple columns in one command)\n",
    "alter_column_comments = []\n",
    "for column, comment in column_comments.items():\n",
    "    # Escape single quotes in comments to prevent SQL injection\n",
    "    escaped_comment = comment.replace(\"'\", \"''\")\n",
    "    alter_column_comments.append(f\"{column} COMMENT '{escaped_comment}'\")\n",
    "\n",
    "# Execute ALTER TABLE with all column comments\n",
    "alter_statement = f\"\"\"\n",
    "    ALTER TABLE {FULL_TABLE_PATH} ALTER COLUMN\n",
    "    {',\\n    '.join(alter_column_comments)}\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(alter_statement)\n",
    "print(f\"‚úì Added descriptions to {len(column_comments)} columns\")\n",
    "\n",
    "# Verify comments were added\n",
    "print(\"\\nColumn descriptions preview:\")\n",
    "describe_df = spark.sql(f\"DESCRIBE TABLE {FULL_TABLE_PATH}\")\n",
    "display(describe_df.select(\"col_name\", \"data_type\", \"comment\").limit(10))\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 2: ADD COLUMN TAGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüè∑Ô∏è  Adding column tags for governance...\")\n",
    "\n",
    "# Define tags for each column\n",
    "# Tags help with data classification, security, and compliance\n",
    "# Common tag categories: PII, Sensitivity, Data Quality, Business Domain\n",
    "\n",
    "column_tags = {\n",
    "    \"year\": {\"category\": \"partition_key\", \"data_classification\": \"public\"},\n",
    "    \"month\": {\"category\": \"partition_key\", \"data_classification\": \"public\"},\n",
    "    \"category\": {\"category\": \"business_dimension\", \"data_classification\": \"public\", \"indexed\": \"z_order\"},\n",
    "    \"region\": {\"category\": \"business_dimension\", \"data_classification\": \"public\", \"indexed\": \"z_order\"},\n",
    "    \"total_transactions\": {\"category\": \"kpi\", \"metric_type\": \"count\", \"data_classification\": \"internal\"},\n",
    "    \"total_quantity_sold\": {\"category\": \"kpi\", \"metric_type\": \"sum\", \"data_classification\": \"internal\"},\n",
    "    \"total_revenue\": {\"category\": \"kpi\", \"metric_type\": \"currency\", \"data_classification\": \"confidential\", \"pii\": \"false\"},\n",
    "    \"avg_transaction_value\": {\"category\": \"kpi\", \"metric_type\": \"average\", \"data_classification\": \"internal\"},\n",
    "    \"max_transaction_value\": {\"category\": \"kpi\", \"metric_type\": \"max\", \"data_classification\": \"internal\"},\n",
    "    \"min_transaction_value\": {\"category\": \"kpi\", \"metric_type\": \"min\", \"data_classification\": \"internal\"},\n",
    "    \"unique_customers\": {\"category\": \"kpi\", \"metric_type\": \"distinct_count\", \"data_classification\": \"internal\"},\n",
    "    \"unique_products\": {\"category\": \"kpi\", \"metric_type\": \"distinct_count\", \"data_classification\": \"internal\"},\n",
    "    \"created_at\": {\"category\": \"metadata\", \"data_classification\": \"public\"},\n",
    "    \"data_source\": {\"category\": \"metadata\", \"data_classification\": \"public\"}\n",
    "}\n",
    "\n",
    "# Apply tags to each column (must be done separately per column)\n",
    "tag_count = 0\n",
    "for column, tags in column_tags.items():\n",
    "    # Convert tags dict to proper format: ('key' = 'value', 'key2' = 'value2')\n",
    "    tag_pairs = [f\"'{k}' = '{v}'\" for k, v in tags.items()]\n",
    "    tags_string = \", \".join(tag_pairs)\n",
    "    \n",
    "    # Apply tags to column\n",
    "    spark.sql(f\"\"\"\n",
    "        ALTER TABLE {FULL_TABLE_PATH} \n",
    "        ALTER COLUMN {column} \n",
    "        SET TAGS ({tags_string})\n",
    "    \"\"\")\n",
    "    tag_count += len(tags)\n",
    "    print(f\"  ‚úì Tagged column '{column}' with {len(tags)} tags\")\n",
    "\n",
    "print(f\"\\n‚úì Applied {tag_count} total tags across {len(column_tags)} columns\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 3: ADD TABLE-LEVEL TAGS\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüè∑Ô∏è  Adding table-level tags...\")\n",
    "\n",
    "# Define table-level tags for governance and classification\n",
    "table_tags = {\n",
    "    \"data_domain\": \"sales_analytics\",\n",
    "    \"data_owner\": \"data_engineering_team\",\n",
    "    \"refresh_frequency\": \"daily\",\n",
    "    \"data_classification\": \"internal\",\n",
    "    \"contains_pii\": \"false\",\n",
    "    \"quality_tier\": \"gold\",\n",
    "    \"business_criticality\": \"high\",\n",
    "    \"retention_policy\": \"3_years\",\n",
    "    \"compliance\": \"gdpr_compliant\",\n",
    "    \"sla\": \"24_hours\",\n",
    "    \"table_type\": \"aggregated\",\n",
    "    \"partition_strategy\": \"year_month\",\n",
    "    \"optimization\": \"z_order_enabled\"\n",
    "}\n",
    "\n",
    "# Convert to proper format\n",
    "table_tag_pairs = [f\"'{k}' = '{v}'\" for k, v in table_tags.items()]\n",
    "table_tags_string = \", \".join(table_tag_pairs)\n",
    "\n",
    "# Apply tags to table\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {FULL_TABLE_PATH} \n",
    "    SET TAGS ({table_tags_string})\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úì Applied {len(table_tags)} tags to table\")\n",
    "print(\"\\nTable tags:\")\n",
    "for key, value in table_tags.items():\n",
    "    print(f\"  ‚Ä¢ {key}: {value}\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 4: UPDATE TABLE DESCRIPTION (EXTENDED COMMENT)\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìÑ Updating table description...\")\n",
    "\n",
    "# Create comprehensive table description with Markdown support\n",
    "table_description = f\"\"\"\n",
    "# Sales Summary Analytics Table\n",
    "\n",
    "## Overview\n",
    "Aggregated sales data providing monthly insights by category and region. \n",
    "This table serves as the primary source for sales performance dashboards and reporting.\n",
    "\n",
    "## Data Characteristics\n",
    "- **Granularity**: Monthly aggregations\n",
    "- **Dimensions**: Year, Month, Category, Region\n",
    "- **Metrics**: Revenue, Transactions, Quantities, Customer counts\n",
    "- **Partitioning**: Year and Month for optimal time-based queries\n",
    "- **Optimization**: Z-ORDER on Category and Region\n",
    "\n",
    "## Business Use Cases\n",
    "1. Monthly sales performance tracking\n",
    "2. Category and regional comparison analysis\n",
    "3. Customer behavior insights\n",
    "4. Revenue forecasting and planning\n",
    "\n",
    "## Technical Details\n",
    "- **Source**: Raw transactional sales data (CSV/Parquet)\n",
    "- **ETL Pipeline**: Automated data cleaning and aggregation\n",
    "- **Update Frequency**: Daily incremental loads\n",
    "- **Data Quality**: Validated and cleaned\n",
    "- **Time Travel**: Enabled (30 days retention)\n",
    "\n",
    "## Data Lineage\n",
    "Raw Sales Data ‚Üí Data Cleaning ‚Üí Aggregation ‚Üí Unity Catalog Table\n",
    "\n",
    "## Owner & Contact\n",
    "- **Team**: Data Engineering\n",
    "- **Contact**: data-eng@company.com\n",
    "- **Last Updated**: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\n",
    "- **Version**: 1.0\n",
    "\n",
    "## Important Notes\n",
    "- Partitioned by year/month - always include these in WHERE clauses for best performance\n",
    "- Z-ORDER optimized for category and region filters\n",
    "- Revenue excludes returns and cancellations\n",
    "- Historical data available via Delta Lake time travel\n",
    "\"\"\"\n",
    "\n",
    "# Apply comprehensive comment to table (supports Markdown)\n",
    "escaped_description = table_description.replace(\"'\", \"''\")\n",
    "spark.sql(f\"\"\"\n",
    "    COMMENT ON TABLE {FULL_TABLE_PATH} IS '{escaped_description}'\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úì Table description updated with comprehensive documentation\")\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 5: VERIFY ALL METADATA\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"METADATA VERIFICATION\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Show table details with comments\n",
    "print(\"\\n1. Table Information:\")\n",
    "table_info = spark.sql(f\"DESCRIBE EXTENDED {FULL_TABLE_PATH}\")\n",
    "display(table_info)\n",
    "\n",
    "# Show column details with comments and data types\n",
    "print(\"\\n2. Column Details with Descriptions:\")\n",
    "column_details = spark.sql(f\"DESCRIBE TABLE {FULL_TABLE_PATH}\")\n",
    "display(column_details)\n",
    "\n",
    "# Query table tags from INFORMATION_SCHEMA\n",
    "print(\"\\n3. Table Tags:\")\n",
    "table_tags_query = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        tag_name,\n",
    "        tag_value\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.TABLE_TAGS\n",
    "    WHERE catalog_name = '{CATALOG_NAME}'\n",
    "    AND schema_name = '{SCHEMA_NAME}'\n",
    "    AND table_name = '{TABLE_NAME}'\n",
    "    ORDER BY tag_name\n",
    "\"\"\")\n",
    "display(table_tags_query)\n",
    "\n",
    "# Query column tags from INFORMATION_SCHEMA\n",
    "print(\"\\n4. Column Tags:\")\n",
    "column_tags_query = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        column_name,\n",
    "        tag_name,\n",
    "        tag_value\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMN_TAGS\n",
    "    WHERE catalog_name = '{CATALOG_NAME}'\n",
    "    AND schema_name = '{SCHEMA_NAME}'\n",
    "    AND table_name = '{TABLE_NAME}'\n",
    "    ORDER BY column_name, tag_name\n",
    "\"\"\")\n",
    "display(column_tags_query)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 6: CREATE METADATA SUMMARY REPORT\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\nüìä Metadata Summary Report:\")\n",
    "\n",
    "# Count total metadata elements\n",
    "total_column_comments = len(column_comments)\n",
    "total_column_tags = tag_count\n",
    "total_table_tags = len(table_tags)\n",
    "\n",
    "print(f\"\\n  Documentation Metrics:\")\n",
    "print(f\"    ‚Ä¢ Columns documented: {total_column_comments}/{len(df_aggregated.columns)}\")\n",
    "print(f\"    ‚Ä¢ Column tags applied: {total_column_tags}\")\n",
    "print(f\"    ‚Ä¢ Table tags applied: {total_table_tags}\")\n",
    "print(f\"    ‚Ä¢ Table description: ‚úì Comprehensive Markdown documentation\")\n",
    "\n",
    "# Create a summary DataFrame\n",
    "metadata_summary = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        '{FULL_TABLE_PATH}' as table_path,\n",
    "        COUNT(DISTINCT c.column_name) as documented_columns,\n",
    "        COUNT(DISTINCT ct.column_name) as tagged_columns,\n",
    "        COUNT(DISTINCT ct.tag_name) as total_column_tags,\n",
    "        '{len(table_tags)}' as table_level_tags\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMNS c\n",
    "    LEFT JOIN {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMN_TAGS ct\n",
    "        ON c.table_catalog = ct.catalog_name\n",
    "        AND c.table_schema = ct.schema_name\n",
    "        AND c.table_name = ct.table_name\n",
    "        AND c.column_name = ct.column_name\n",
    "    WHERE c.table_catalog = '{CATALOG_NAME}'\n",
    "    AND c.table_schema = '{SCHEMA_NAME}'\n",
    "    AND c.table_name = '{TABLE_NAME}'\n",
    "\"\"\")\n",
    "\n",
    "display(metadata_summary)\n",
    "\n",
    "# ============================================================================\n",
    "# STEP 7: METADATA BEST PRACTICES VALIDATION\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n‚úÖ Metadata Best Practices Check:\")\n",
    "\n",
    "# Check 1: All columns have comments\n",
    "columns_without_comments = spark.sql(f\"\"\"\n",
    "    SELECT column_name\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMNS\n",
    "    WHERE table_catalog = '{CATALOG_NAME}'\n",
    "    AND table_schema = '{SCHEMA_NAME}'\n",
    "    AND table_name = '{TABLE_NAME}'\n",
    "    AND (comment IS NULL OR comment = '')\n",
    "\"\"\").count()\n",
    "\n",
    "if columns_without_comments == 0:\n",
    "    print(\"  ‚úì All columns have descriptions\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  {columns_without_comments} columns missing descriptions\")\n",
    "\n",
    "# Check 2: Critical columns have tags\n",
    "critical_columns = ['total_revenue', 'unique_customers']\n",
    "tagged_critical = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT column_name\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMN_TAGS\n",
    "    WHERE catalog_name = '{CATALOG_NAME}'\n",
    "    AND schema_name = '{SCHEMA_NAME}'\n",
    "    AND table_name = '{TABLE_NAME}'\n",
    "    AND column_name IN ('total_revenue', 'unique_customers')\n",
    "\"\"\").count()\n",
    "\n",
    "if tagged_critical == len(critical_columns):\n",
    "    print(f\"  ‚úì All critical columns are tagged\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Some critical columns need tags\")\n",
    "\n",
    "# Check 3: Table has governance tags\n",
    "governance_tags = ['data_classification', 'data_owner', 'contains_pii']\n",
    "has_governance = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT tag_name) as gov_tag_count\n",
    "    FROM {CATALOG_NAME}.INFORMATION_SCHEMA.TABLE_TAGS\n",
    "    WHERE catalog_name = '{CATALOG_NAME}'\n",
    "    AND schema_name = '{SCHEMA_NAME}'\n",
    "    AND table_name = '{TABLE_NAME}'\n",
    "    AND tag_name IN ('data_classification', 'data_owner', 'contains_pii')\n",
    "\"\"\").collect()[0].gov_tag_count\n",
    "\n",
    "if has_governance >= 3:\n",
    "    print(f\"  ‚úì Table has all required governance tags\")\n",
    "else:\n",
    "    print(f\"  ‚ö†Ô∏è  Table missing some governance tags\")\n",
    "\n",
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ METADATA ENRICHMENT COMPLETED\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìã Summary:\")\n",
    "print(f\"  ‚Ä¢ Table: {FULL_TABLE_PATH}\")\n",
    "print(f\"  ‚Ä¢ Column descriptions: {total_column_comments} added\")\n",
    "print(f\"  ‚Ä¢ Column tags: {total_column_tags} applied\")\n",
    "print(f\"  ‚Ä¢ Table tags: {total_table_tags} applied\")\n",
    "print(f\"  ‚Ä¢ Table documentation: ‚úì Comprehensive Markdown description\")\n",
    "\n",
    "print(f\"\\nüéØ Benefits:\")\n",
    "print(f\"  ‚úì Improved data discoverability in Catalog Explorer\")\n",
    "print(f\"  ‚úì Enhanced data governance and compliance tracking\")\n",
    "print(f\"  ‚úì Better data lineage and ownership transparency\")\n",
    "print(f\"  ‚úì Easier onboarding for new data consumers\")\n",
    "print(f\"  ‚úì Automated data classification for security policies\")\n",
    "\n",
    "print(f\"\\nüìñ View Metadata:\")\n",
    "print(f\"  ‚Ä¢ Catalog Explorer: Browse to {FULL_TABLE_PATH}\")\n",
    "print(f\"  ‚Ä¢ SQL: DESCRIBE EXTENDED {FULL_TABLE_PATH}\")\n",
    "print(f\"  ‚Ä¢ SQL: SELECT * FROM {CATALOG_NAME}.INFORMATION_SCHEMA.TABLE_TAGS\")\n",
    "print(f\"  ‚Ä¢ SQL: SELECT * FROM {CATALOG_NAME}.INFORMATION_SCHEMA.COLUMN_TAGS\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e81dd8ae-5806-4006-b79b-595a09eedd11",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# FINAL SUMMARY\n",
    "# ============================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"ETL PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Summary:\")\n",
    "print(f\"  ‚Ä¢ Source: Raw CSV/Parquet files\")\n",
    "print(f\"  ‚Ä¢ Destination: {FULL_TABLE_PATH}\")\n",
    "print(f\"  ‚Ä¢ Records processed: {df_raw.count():,}\")\n",
    "print(f\"  ‚Ä¢ Records loaded: {table_count:,}\")\n",
    "print(f\"  ‚Ä¢ Data quality: ‚úì Cleaned and validated\")\n",
    "print(f\"  ‚Ä¢ Format: Delta Lake\")\n",
    "print(f\"  ‚Ä¢ Governance: Unity Catalog\")\n",
    "print(f\"  ‚Ä¢ Start Execution DateTime: {start_notebook_datetime}\")\n",
    "print(f\"  ‚Ä¢ End Execution DateTime: {end_notebook_datetime}\")\n",
    "print(f\"  ‚Ä¢ Execution Time: {int(hours)}h {int(minutes)}m {seconds:.2f}s\")\n",
    "print(f\"\\n‚úÖ Data is now available for analytics and reporting\")\n",
    "print(f\"\\nNext steps:\")\n",
    "print(f\"  1. Query the table: SELECT * FROM {FULL_TABLE_PATH}\")\n",
    "print(f\"  2. Create dashboards using Databricks SQL\")\n",
    "print(f\"  3. Set up automated jobs for regular updates\")\n",
    "print(f\"  4. Configure alerts for data quality monitoring\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Raw data to Aggregate Data",
   "widgets": {
    "": {
     "currentValue": "sales_summary",
     "nuid": "bdd260b0-0ecb-4d3a-ac13-5e915355666d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sales_summary",
      "label": null,
      "name": "",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sales_summary",
      "label": null,
      "name": "",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "CATALOG_NAME": {
     "currentValue": "workspace",
     "nuid": "f22e09ea-d33e-4450-a713-38ead9479412",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": null,
      "name": "CATALOG_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "SCHEMA_NAME": {
     "currentValue": "portfolio_projects",
     "nuid": "4a8b1146-f741-4187-ad81-e54b1599f703",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "portfolio_project",
      "label": null,
      "name": "SCHEMA_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "TABLE_NAME": {
     "currentValue": "sales_summary",
     "nuid": "7d606ef1-28d4-4467-b99e-812490b6a637",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "sales_summary",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "sales_summary",
      "label": null,
      "name": "TABLE_NAME",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
